
<!DOCTYPE HTML>
<html lang="en">
<head>
    <title>awesome-paper-list</title>
    <link rel="shortcut icon" href="assets/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>
<table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr style="padding:0px">
    <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>awesome-paper-list</h2>
            </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <!-- KerblGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gaussian Splatting for Real-Time Radiance Field Rendering</span>
                        <p style="margin:0">Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis</p>
                        <em>None, None</em>
                        <br>
                        <a href='https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/'>project page</a> / <a href='https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/3d_gaussian_splatting_low.pdf'>paper</a> / <a href='https://github.com/graphdeco-inria/gaussian-splatting'>code</a> / <a href='https://youtu.be/T_kXY43VZnk?si=DrkbDFxQAv5scQNT'>video</a>
                        <p>Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos.</p>
                    </td>
                </tr>
                <!-- Yan2024Street -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Street Gaussians for Modeling Dynamic Urban Scenes</span>
                        <p style="margin:0">Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, Sida Peng</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://zju3dv.github.io/street_gaussians/'>project page</a> / <a href='https://arxiv.org/pdf/2401.01339.pdf'>paper</a> / <a href='https://github.com/zju3dv/street_gaussians'>code</a>
                        <p>This paper aims to tackle the problem of modeling dynamic urban street scenes from monocular videos.</p>
                    </td>
                </tr>
                <!-- Zhou2023DrivingGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes</span>
                        <p style="margin:0">Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://pkuvdig.github.io/DrivingGaussian/'>project page</a> / <a href='https://arxiv.org/pdf/2312.07920.pdf'>paper</a>
                        <p>We present DrivingGaussian, an efficient and effective framework for surrounding dynamic autonomous driving scenes.</p>
                    </td>
                </tr>
                <!-- Li2024GaussianBody -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting</span>
                        <p style="margin:0">Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen, Yu-Gang Jiang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2401.09720.pdf'>paper</a>
                        <p>In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting.</p>
                    </td>
                </tr>
                <!-- Zhao2024PSAvatar -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Creation with 3D Gaussian Splatting</span>
                        <p style="margin:0">Zhongyuan Zhao, Zhenyu Bao, Qing Li, Guoping Qiu, Kanglin Liu</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2401.12900.pdf'>paper</a>
                        <p>Despite much progress, achieving real-time high-fidelity head avatar animation is still difficult and existing methods have to trade-off between speed and quality.</p>
                    </td>
                </tr>
                <!-- Rivero2024Rig3DGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos</span>
                        <p style="margin:0">Alfredo Rivero, ShahRukh Athar, Zhixin Shu, Dimitris Samaras</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html'>project page</a> / <a href='https://arxiv.org/pdf/2402.03723.pdf'>paper</a>
                        <p>Creating controllable 3D human portraits from casual smartphone videos is highly desirable due to their immense value in AR/VR applications.</p>
                    </td>
                </tr>
                <!-- Zhou2024HeadStudio -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting</span>
                        <p style="margin:0">Zhenglin Zhou, Fan Ma, Hehe Fan, Yi Yang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://zhenglinzhou.github.io/HeadStudio-ProjectPage/'>project page</a> / <a href='https://arxiv.org/pdf/2402.06149.pdf'>paper</a> / <a href='https://github.com/ZhenglinZhou/HeadStudio/'>code</a>
                        <p>Creating digital avatars from textual prompts has long been a desirable yet challenging task.</p>
                    </td>
                </tr>
                <!-- Stanishevskii2024ImplicitDeepfake -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">ImplicitDeepfake: Plausible Face-Swapping through Implicit Deepfake Generation using NeRF and Gaussian Splatting</span>
                        <p style="margin:0">Georgii Stanishevskii, Jakub Steczkiewicz, Tomasz Szczepanik, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2402.06390.pdf'>paper</a> / <a href='https://github.com/quereste/implicit-deepfake'>code</a>
                        <p>Numerous emerging deep-learning techniques have had a substantial impact on computer graphics.</p>
                    </td>
                </tr>
                <!-- Luo2024GaussianHair -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians</span>
                        <p style="margin:0">Haimin Luo, Min Ouyang, Zijun Zhao, Suyi Jiang, Longwen Zhang, Qixuan Zhang, Wei Yang, Lan Xu, Jingyi Yu</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2402.10483.pdf'>paper</a>
                        <p>Hairstyle reflects culture and ethnicity at first glance.</p>
                    </td>
                </tr>
                <!-- Liu2024GVA -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos</span>
                        <p style="margin:0">Xinqi Liu, Chenming Wu, Jialun Liu, Xing Liu, Jinbo Wu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://3d-aigc.github.io/GEA/'>project page</a> / <a href='https://arxiv.org/pdf/2402.16607.pdf'>paper</a>
                        <p>In this paper, we present a novel method that facilitates the creation of vivid 3D Gaussian avatars from monocular video inputs (GVA).</p>
                    </td>
                </tr>
                <!-- Shao2024SplattingAvatar -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting</span>
                        <p style="margin:0">Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang</p>
                        <em>CVPR '24, 2024</em>
                        <br>
                        <a href='https://initialneil.github.io/SplattingAvatar'>project page</a> / <a href='https://arxiv.org/pdf/2403.05087.pdf'>paper</a> / <a href='https://github.com/initialneil/SplattingAvatar'>code</a> / <a href='https://www.youtube.com/watch?v=IzC-fLvdntA'>video</a>
                        <p>We present SplattingAvatar, a hybrid 3D representation of photorealistic human avatars with Gaussian Splatting embedded on a triangle mesh, which renders over 300 FPS on a modern GPU and 30 FPS on a mobile device.</p>
                    </td>
                </tr>
                <!-- Shao2024SplatFace -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable Surface</span>
                        <p style="margin:0">Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.18784'>paper</a>
                        <p>We present SplatFace, a novel Gaussian splatting framework designed for 3D human face reconstruction without reliance on accurate pre-determined geometry.</p>
                    </td>
                </tr>
                <!-- Shao2024HAHA -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</span>
                        <p style="margin:0">Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2404.01053'>paper</a>
                        <p>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos.</p>
                    </td>
                </tr>
                <!-- Zielonka2023Drivable -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Drivable 3D Gaussian Avatars</span>
                        <p style="margin:0">Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito, Michael Zollhöfer, Justus Thies, Javier Romero</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://zielon.github.io/d3ga/'>project page</a> / <a href='https://arxiv.org/pdf/2311.08581.pdf'>paper</a> / <a href='https://youtu.be/C4IT1gnkaF0?si=zUJLm8adM68pVvR8'>video</a>
                        <p>We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable model for human bodies rendered with Gaussian splats.</p>
                    </td>
                </tr>
                <!-- Jena2023SplatArmor -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SplatArmor: Articulated Gaussian splatting for animatable humans from monocular RGB videos</span>
                        <p style="margin:0">Rohit Jena, Ganesh Subramanian Iyer, Siddharth Choudhary, Brandon Smith, Pratik Chaudhari, James Gee</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://jenaroh.it/splatarmor/'>project page</a> / <a href='https://arxiv.org/pdf/2311.10812.pdf'>paper</a> / <a href='https://github.com/rohitrango/splatarmor'>code</a>
                        <p>We propose SplatArmor, a novel approach for recovering detailed and animatable human models by `armoring' a parameterized body model with 3D Gaussians.</p>
                    </td>
                </tr>
                <!-- Li2023Animatable -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling</span>
                        <p style="margin:0">Zhe Li, Zerong Zheng, Lizhen Wang, Yebin Liu</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://animatable-gaussians.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2311.16096.pdf'>paper</a> / <a href='https://github.com/lizhe00/AnimatableGaussians'>code</a>
                        <p>Modeling animatable human avatars from RGB videos is a long-standing and challenging problem.</p>
                    </td>
                </tr>
                <!-- Lei2023GART -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GART: Gaussian Articulated Template Models</span>
                        <p style="margin:0">Jiahui Lei, Yufu Wang, Georgios Pavlakos, Lingjie Liu, Kostas Daniilidis</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://www.cis.upenn.edu/~leijh/projects/gart/'>project page</a> / <a href='https://arxiv.org/pdf/2311.16099.pdf'>paper</a> / <a href='https://github.com/JiahuiLei/GART'>code</a> / <a href='https://www.youtube.com/watch?v=-xYNtIlW4WY'>video</a>
                        <p>We introduce Gaussian Articulated Template Model GART, an explicit, efficient, and expressive representation for non-rigid articulated subject capturing and rendering from monocular videos.</p>
                    </td>
                </tr>
                <!-- Moreau2023Human -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Human Gaussian Splatting: Real-time Rendering of Animatable Avatars</span>
                        <p style="margin:0">Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw, Yiren Zhou, Eduardo Pérez-Pellitero</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2311.17113.pdf'>paper</a>
                        <p>This work addresses the problem of real-time rendering of photorealistic human body avatars learned from multi-view videos.</p>
                    </td>
                </tr>
                <!-- Kocabas2023HUGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">HUGS: Human Gaussian Splats</span>
                        <p style="margin:0">Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag Ranjan</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://machinelearning.apple.com/research/hugs'>project page</a> / <a href='https://arxiv.org/pdf/2311.17910.pdf'>paper</a> / <a href='https://github.com/apple/ml-hugs'>code</a>
                        <p>Recent advances in neural rendering have improved both training and rendering times by orders of magnitude.</p>
                    </td>
                </tr>
                <!-- Abdal2023Gaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gaussian Shell Maps for Efficient 3D Human Generation</span>
                        <p style="margin:0">Rameen Abdal, Wang Yifan, Zifan Shi, Yinghao Xu, Ryan Po, Zhengfei Kuang, Qifeng Chen, Dit-Yan Yeung, Gordon Wetzstein</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://rameenabdal.github.io/GaussianShellMaps/'>project page</a> / <a href='https://arxiv.org/pdf/2311.17857'>paper</a> / <a href='https://github.com/computational-imaging/GSM'>code</a>
                        <p>Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production.</p>
                    </td>
                </tr>
                <!-- Wang2023GaussianHead -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianHead: High-fidelity Head Avatars with Learnable Gaussian Derivation</span>
                        <p style="margin:0">Jie Wang, Jiu-Cheng Xie, Xianyan Li, Chi-Man Pun, Feng Xu, Hao Gao</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://chiehwangs.github.io/gaussian-head-page/'>project page</a> / <a href='https://arxiv.org/pdf/2312.01632.pdf'>paper</a> / <a href='https://github.com/chiehwangs/gaussian-head'>code</a>
                        <p>Constructing vivid 3D head avatars for given subjects and realizing a series of animations on them is valuable yet challenging.</p>
                    </td>
                </tr>
                <!-- Qian2023GaussianAvatars -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians</span>
                        <p style="margin:0">Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, Matthias Nießner</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://shenhanqian.github.io/gaussian-avatars'>project page</a> / <a href='https://arxiv.org/pdf/2312.02069'>paper</a> / <a href='https://github.com/ShenhanQian/GaussianAvatars'>code</a> / <a href='https://youtu.be/lVEY78RwU_I'>video</a>
                        <p>We introduce GaussianAvatars, a new method to create photorealistic head avatars that are fully controllable in terms of expression, pose, and viewpoint.</p>
                    </td>
                </tr>
                <!-- Zheng2023GPSGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis</span>
                        <p style="margin:0">Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, Yebin Liu</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://github.com/ShunyuanZheng/GPS-Gaussian'>project page</a> / <a href='https://arxiv.org/pdf/2312.02155.pdf'>paper</a> / <a href='https://github.com/ShunyuanZheng/GPS-Gaussian'>code</a> / <a href='https://youtu.be/TBIekcqt0j0'>video</a>
                        <p>We present a new approach, termed GPS-Gaussian, for synthesizing novel views of a character in a real-time manner.</p>
                    </td>
                </tr>
                <!-- Hu2023GauHuman -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GauHuman: Articulated Gaussian Splatting from Monocular Human Videos</span>
                        <p style="margin:0">Shoukang Hu Ziwei Liu</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://skhu101.github.io/GauHuman/'>project page</a> / <a href='https://arxiv.org/pdf/2312.02973.pdf'>paper</a> / <a href='https://github.com/skhu101/GauHuman'>code</a> / <a href='https://www.youtube.com/embed/47772bgt5Xo'>video</a>
                        <p>We present, GauHuman, a 3D human model with Gaussian Splatting for both fast training (1~2 minutes) and real-time rendering (up to 189 FPS), compared with existing NeRF-based implicit representation modelling frameworks demanding hours of training and seconds of rendering per frame.</p>
                    </td>
                </tr>
                <!-- Dhamo2023HeadGaS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting</span>
                        <p style="margin:0">Helisa Dhamo, Yinyu Nie, Arthur Moreau, Jifei Song, Richard Shaw, Yiren Zhou, Eduardo Pérez-Pellitero</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2312.02902.pdf'>paper</a>
                        <p>3D head animation has seen major quality and runtime improvements over the last few years, particularly empowered by the advances in differentiable rendering and neural radiance fields.</p>
                    </td>
                </tr>
                <!-- Jiang2023HiFi4G -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting</span>
                        <p style="margin:0">Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, Lan Xu</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://nowheretrix.github.io/HiFi4G/'>project page</a> / <a href='https://arxiv.org/pdf/2312.03461.pdf'>paper</a> / <a href='https://youtu.be/917WVr2EHh4'>video</a>
                        <p>We have recently seen tremendous progress in photo-real human modeling and rendering.</p>
                    </td>
                </tr>
                <!-- Hu2023GaussianAvatar -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</span>
                        <p style="margin:0">Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao Zhou, Boning Liu, Shengping Zhang, Liqiang Nie</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://huliangxiao.github.io/GaussianAvatar'>project page</a> / <a href='https://arxiv.org/pdf/2312.02134.pdf'>paper</a> / <a href='https://github.com/huliangxiao/GaussianAvatar'>code</a> / <a href='https://www.youtube.com/watch?v=a4g8Z9nCF-k&t=1s'>video</a>
                        <p>We present GaussianAvatar, an efficient approach to creating realistic human avatars with dynamic 3D appearances from a single video.</p>
                    </td>
                </tr>
                <!-- Xiang2023FlashAvatar -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">FlashAvatar: High-Fidelity Digital Avatar Rendering at 300FPS</span>
                        <p style="margin:0">Jun Xiang, Xuan Gao, Yudong Guo, Juyong Zhang</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://ustc3dv.github.io/FlashAvatar/'>project page</a> / <a href='https://arxiv.org/pdf/2312.02134.pdf'>paper</a>
                        <p>We propose FlashAvatar, a novel and lightweight 3D animatable avatar representation that could reconstruct a digital avatar from a short monocular video sequence in minutes and render high-fidelity photo-realistic images at 300FPS on a consumer-grade GPU.</p>
                    </td>
                </tr>
                <!-- Saito2023Relightable -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Relightable Gaussian Codec Avatars</span>
                        <p style="margin:0">Shunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, Giljoo Nam</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://shunsukesaito.github.io/rgca/'>project page</a> / <a href='https://arxiv.org/pdf/2312.03704.pdf'>paper</a>
                        <p>The fidelity of relighting is bounded by both geometry and appearance representations.</p>
                    </td>
                </tr>
                <!-- Chen2023MonoGaussianAvatar -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar</span>
                        <p style="margin:0">Yufan Chen, Lizhen Wang, Qijing Li, Hongjiang Xiao, Shengping Zhang, Hongxun Yao, Yebin Liu</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://yufan1012.github.io/MonoGaussianAvatar'>project page</a> / <a href='https://arxiv.org/pdf/2312.04558.pdf'>paper</a> / <a href='https://github.com/yufan1012/MonoGaussianAvatar'>code</a> / <a href='https://youtu.be/3UvBkyPc-oc?si=SbveQKBLJh5GuhIY'>video</a>
                        <p>The ability to animate photo-realistic head avatars reconstructed from monocular portrait video sequences represents a crucial step in bridging the gap between the virtual and real worlds.</p>
                    </td>
                </tr>
                <!-- Pang2023ASH -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering</span>
                        <p style="margin:0">Haokai Pang, Heming Zhu, Adam Kortylewski, Christian Theobalt, Marc Habermann</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://vcai.mpi-inf.mpg.de/projects/ash/'>project page</a> / <a href='https://arxiv.org/pdf/2312.05941.pdf'>paper</a> / <a href='https://vcai.mpi-inf.mpg.de/projects/ash/videos/video_for_page.mp4'>video</a>
                        <p>Real-time rendering of photorealistic and controllable human avatars stands as a cornerstone in Computer Vision and Graphics.</p>
                    </td>
                </tr>
                <!-- Qian20233DGSAvatar -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</span>
                        <p style="margin:0">Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, Siyu Tang</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://neuralbodies.github.io/3DGS-Avatar/index.html'>project page</a> / <a href='https://arxiv.org/pdf/2312.09228.pdf'>paper</a> / <a href='https://github.com/mikeqzy/3dgs-avatar-release'>code</a> / <a href='https://youtu.be/FJ29U9OkmmU?si=5ua2mtpv5ei2n28Z'>video</a>
                        <p>We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS).</p>
                    </td>
                </tr>
                <!-- Yuan2023GAvatar -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning</span>
                        <p style="margin:0">Ye Yuan, Xueting Li, Yangyi Huang, Shalini De Mello, Koki Nagano, Jan Kautz, Umar Iqbal</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://nvlabs.github.io/GAvatar/'>project page</a> / <a href='https://arxiv.org/pdf/2312.11461.pdf'>paper</a> / <a href='https://www.youtube.com/watch?v=PbCF1HzrKrs'>video</a>
                        <p>Gaussian splatting has emerged as a powerful 3D representation that harnesses the advantages of both explicit (mesh) and implicit (NeRF) 3D representations.</p>
                    </td>
                </tr>
                <!-- Jung2023Deformable -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Deformable 3D Gaussian Splatting for Animatable Human Avatars</span>
                        <p style="margin:0">HyunJun Jung, Nikolas Brasch, Jifei Song, Eduardo Perez-Pellitero, Yiren Zhou, Zhihao Li, Nassir Navab, Benjamin Busam</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2312.15059.pdf'>paper</a>
                        <p>Recent advances in neural radiance fields enable novel view synthesis of photo-realistic images in dynamic settings, which can be applied to scenarios with human animation.</p>
                    </td>
                </tr>
                <!-- Li2023Human101 -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Human101: Training 100+FPS Human Gaussians in 100s from 1 View</span>
                        <p style="margin:0">Mingwei Li, Jiachen Tao, Zongxin Yang, Yi Yang</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://longxiang-ai.github.io/Human101/'>project page</a> / <a href='https://arxiv.org/pdf/2312.15258.pdf'>paper</a> / <a href='https://github.com/longxiang-ai/Human101'>code</a>
                        <p>Reconstructing the human body from single-view videos plays a pivotal role in the virtual reality domain.</p>
                    </td>
                </tr>
                <!-- Xu2023Gaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians</span>
                        <p style="margin:0">Yuelang Xu, Benwang Chen, Zhe Li, Hongwen Zhang, Lizhen Wang, Zerong Zheng, Yebin Liu</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://yuelangx.github.io/gaussianheadavatar/'>project page</a> / <a href='https://arxiv.org/pdf/2312.03029.pdf'>paper</a> / <a href='https://github.com/YuelangX/Gaussian-Head-Avatar'>code</a> / <a href='https://www.youtube.com/watch?v=kvrrI3EoM5g'>video</a>
                        <p>Creating high-fidelity 3D head avatars has always been a research hotspot, but there remains a great challenge under lightweight sparse view setups.</p>
                    </td>
                </tr>
                <!-- F2023A -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">A Generalization of Algebraic Surface Drawing</span>
                        <p style="margin:0">James F. Blinn</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://dl.acm.org/doi/pdf/10.1145/357306.357310'>paper</a>
                        <p>The mathematical description of three-dimensional surfaces usually falls into one of two classifications:  parametric and implicit.</p>
                    </td>
                </tr>
                <!-- Keselman2023Approximate -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Approximate Differentiable Rendering with Algebraic Surfaces</span>
                        <p style="margin:0">Leonid Keselman and Martial Hebert</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://leonidk.com/fuzzy-metaballs/'>project page</a> / <a href='https://arxiv.org/pdf/2207.10606.pdf'>paper</a> / <a href='https://github.com/leonidk/fuzzy-metaballs'>code</a> / <a href='https://www.youtube.com/watch?v=Ec7cxEc9eOU'>video</a>
                        <p>Differentiable renderers provide a direct mathematical link between an object’s 3D representation and images of that object.</p>
                    </td>
                </tr>
                <!-- U2023Unbiased -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Unbiased Gradient Estimation for Differentiable Surface Splatting via Poisson Sampling</span>
                        <p style="margin:0">Jan U. Müller, Michael Weinmann, Reinhard Klein</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930276.pdf'>paper</a> / <a href='https://github.com/muellerju/unbiased-differentiable-splatting'>code</a>
                        <p>We propose an efficient and GPU-accelerated sampling framework which enables unbiased gradient approximation for differentiable point cloud rendering based on surface splatting.</p>
                    </td>
                </tr>
                <!-- Man2023Generating -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Generating and Real-Time Rendering of Clouds</span>
                        <p style="margin:0">Petr Man</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://old.cescg.org/CESCG-2006/papers/Prague-Man-Petr.pdf'>paper</a>
                        <p>This paper presents a method for generation and real-time rendering of static clouds.</p>
                    </td>
                </tr>
                <!-- Niedermayr2024Compressed -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis</span>
                        <p style="margin:0">Simon Niedermayr, Josef Stumpfegger, Rüdiger Westermann</p>
                        <em>CVPR '24, 2024</em>
                        <br>
                        <a href='https://keksboter.github.io/c3dgs/'>project page</a> / <a href='https://arxiv.org/pdf/2401.02436.pdf'>paper</a> / <a href='https://github.com/KeKsBoTer/c3dgs'>code</a>
                        <p>Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian splat representation has been introduced for novel view synthesis from sparse image sets.</p>
                    </td>
                </tr>
                <!-- Chen2024HAC -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression</span>
                        <p style="margin:0">Yihang Chen, Qianyi Wu, Jianfei Cai, Mehrtash Harandi, Weiyao Lin</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://yihangchen-ee.github.io/project_hac/'>project page</a> / <a href='https://arxiv.org/pdf/2401.02436.pdf'>paper</a> / <a href='https://github.com/YihangChen-ee/HAC'>code</a>
                        <p>3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity.</p>
                    </td>
                </tr>
                <!-- Fan2023LightGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</span>
                        <p style="margin:0">Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://lightgaussian.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2311.17245.pdf'>paper</a> / <a href='https://github.com/VITA-Group/LightGaussian'>code</a> / <a href='https://youtu.be/470hul75bSM?si=EKm-UaBaTs9qJH6K'>video</a>
                        <p>Recent advancements in real-time neural rendering using point-based techniques have paved the way for the widespread adoption of 3D representations.</p>
                    </td>
                </tr>
                <!-- Navaneet2023Compact3D -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Compact3D: Compressing Gaussian Splat Radiance Field Models with Vector Quantization</span>
                        <p style="margin:0">KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, Hamed Pirsiavash</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2311.18159.pdf'>paper</a> / <a href='https://github.com/UCDvision/compact3d'>code</a>
                        <p>3D Gaussian Splatting is a new method for modeling and rendering 3D radiance fields that achieves much faster learning and rendering time compared to SOTA NeRF methods.</p>
                    </td>
                </tr>
                <!-- Chan2023Compact -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Compact 3D Gaussian Representation for Radiance Field</span>
                        <p style="margin:0">Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, Eunbyung Park</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://maincold2.github.io/c3dgs/'>project page</a> / <a href='https://arxiv.org/pdf/2311.13681.pdf'>paper</a> / <a href='https://github.com/maincold2/Compact-3DGS'>code</a>
                        <p>Neural Radiance Fields (NeRFs) have demonstrated remarkable potential in capturing complex 3D scenes with high fidelity.</p>
                    </td>
                </tr>
                <!-- Morgenstern2023Compact -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Compact 3D Scene Representation via Self-Organizing Gaussian Grids</span>
                        <p style="margin:0">Wieland Morgenstern, Florian Barthel, Anna Hilsmann, Peter Eisert</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://fraunhoferhhi.github.io/Self-Organizing-Gaussians/'>project page</a> / <a href='https://arxiv.org/pdf/2312.13299.pdf'>paper</a> / <a href='https://github.com/fraunhoferhhi/Self-Organizing-Gaussians'>code</a>
                        <p>3D Gaussian Splatting has recently emerged as a highly promising technique for modeling of static 3D scenes.</p>
                    </td>
                </tr>
                <!-- Xu2024AGG -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">AGG: Amortized Generative 3D Gaussians for Single Image to 3D</span>
                        <p style="margin:0">Dejia Xu, Ye Yuan, Morteza Mardani, Sifei Liu, Jiaming Song, Zhangyang Wang, Arash Vahdat</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://ir1d.github.io/AGG/'>project page</a> / <a href='https://arxiv.org/pdf/2401.04099.pdf'>paper</a> / <a href='https://youtu.be/jkwmp2UH0Ug?si=lBXjme-d9bVrXTNf'>video</a>
                        <p>Given the growing need for automatic 3D content creation pipelines, various 3D representations have been studied to generate 3D objects from a single image.</p>
                    </td>
                </tr>
                <!-- Pan2024Fast -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Fast Dynamic 3D Object Generation from a Single-view Video</span>
                        <p style="margin:0">Zijie Pan, Zeyu Yang, Xiatian Zhu, Li Zhang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://fudan-zvg.github.io/Efficient4D/'>project page</a> / <a href='https://arxiv.org/pdf/2401.08742.pdf'>paper</a> / <a href='https://github.com/fudan-zvg/Efficient4D'>code</a> / <a href='https://fudan-zvg.github.io/Efficient4D/assets/video/demo.mp4'>video</a>
                        <p>Generating dynamic three-dimensional (3D) object from a single-view video is challenging due to the lack of 4D labeled data.</p>
                    </td>
                </tr>
                <!-- Yang2024GaussianObject -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting</span>
                        <p style="margin:0">Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://gaussianobject.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2402.10259.pdf'>paper</a> / <a href='https://github.com/GaussianObject/GaussianObject'>code</a> / <a href='https://youtu.be/ozoI0tmW3r0?si=KcaHtvVnrexqaf58'>video</a>
                        <p>Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience.</p>
                    </td>
                </tr>
                <!-- Yang2024Large -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Large Multi-View Gaussian Model for High-Resolution 3D Content Creation</span>
                        <p style="margin:0">Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://me.kiui.moe/lgm/'>project page</a> / <a href='https://arxiv.org/pdf/2402.05054.pdf'>paper</a> / <a href='https://github.com/3DTopia/LGM'>code</a>
                        <p>Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience.</p>
                    </td>
                </tr>
                <!-- Zhou2024GALA3D -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting</span>
                        <p style="margin:0">Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://gala3d.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2402.07207.pdf'>paper</a> / <a href='https://github.com/VDIGPKU/GALA3D'>code</a>
                        <p>We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation.</p>
                    </td>
                </tr>
                <!-- MelasKyriazi2024IM3D -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation</span>
                        <p style="margin:0">Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, Filippos Kokkinos</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2402.08682.pdf'>paper</a>
                        <p>Most text-to-3D generators build upon off-the-shelf text-to-image models trained on billions of images.</p>
                    </td>
                </tr>
                <!-- Li2024Controllable -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting</span>
                        <p style="margin:0">Zhiqi Li, Yiming Chen, Lingzhe Zhao, Peidong Liu</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://lizhiqi49.github.io/MVControl/'>project page</a> / <a href='https://lizhiqi49.github.io/MVControl/assets/paper.pdf'>paper</a> / <a href='https://github.com/WU-CVGL/MVControl-threestudio'>code</a>
                        <p>While text-to-3D and image-to-3D generation tasks have received considerable attention, one important but under-explored field between them is controllable text-to-3D generation, which we mainly focus on in this work.</p>
                    </td>
                </tr>
                <!-- Di2024Hyper3DGTextto3D -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Hyper-3DG:Text-to-3D Gaussian Generation via Hypergraph</span>
                        <p style="margin:0">Donglin Di, Jiahui Yang, Chaofan Luo, Zhou Xue, Wei Chen, Xun Yang, Yue Gao</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.09236.pdf'>paper</a> / <a href='https://github.com/yjhboy/Hyper3DG'>code</a>
                        <p>Text-to-3D generation represents an exciting field that has seen rapid advancements, facilitating the transformation of textual descriptions into detailed 3D models.</p>
                    </td>
                </tr>
                <!-- Li2024DreamScene -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation Pattern Sampling</span>
                        <p style="margin:0">Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik Hang Lee, Pengyuan Zhou</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://dreamscene-project.github.io/'>project page</a>
                        <p>Text-to-3D scene generation holds immense potential for the gaming, film, and architecture sectors, increasingly capturing the attention of both academic and industry circles.</p>
                    </td>
                </tr>
                <!-- Feng2024FDGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model</span>
                        <p style="margin:0">Qijun Feng, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://qjfeng.net/FDGaussian'>project page</a> / <a href='https://arxiv.org/pdf/2403.10242.pdf'>paper</a>
                        <p>Reconstructing detailed 3D objects from single-view images remains a challenging task due to the limited information available.</p>
                    </td>
                </tr>
                <!-- Zhang2024BAGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">BAGS: Building Animatable Gaussian Splatting from a Monocular Video with Diffusion Priors</span>
                        <p style="margin:0">Tingyang Zhang, Qingzhe Gao, Weiyu Li, Libin Liu, Baoquan Chen</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.11427'>paper</a>
                        <p>Animatable 3D reconstruction has significant applications across various fields, primarily relying on artists' handcraft creation.</p>
                    </td>
                </tr>
                <!-- Jiang2024BrightDreamer -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">BrightDreamer: Generic 3D Gaussian Generative Framework for Fast Text-to-3D Synthesis</span>
                        <p style="margin:0">Lutao Jiang, Lin Wang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://vlislab22.github.io/BrightDreamer/'>project page</a> / <a href='https://arxiv.org/pdf/2403.11273'>paper</a> / <a href='https://github.com/lutao2021/BrightDreamer'>code</a>
                        <p>Text-to-3D synthesis has recently seen intriguing advances by combining the text-to-image models with 3D representation methods, e.</p>
                    </td>
                </tr>
                <!-- He2024GVGEN -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GVGEN: Text-to-3D Generation with Volumetric Representation</span>
                        <p style="margin:0">Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, Tong He</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://gvgen.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2403.12957'>paper</a> / <a href='https://github.com/GVGEN/GVGEN'>code</a>
                        <p>In recent years, 3D Gaussian splatting has emerged as a powerful technique for 3D reconstruction and generation, known for its fast and high-quality rendering capabilities.</p>
                    </td>
                </tr>
                <!-- Kim2024SyncTweedies -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SyncTweedies: A General Generative Framework Based on Synchronized Diffusions</span>
                        <p style="margin:0">Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://synctweedies.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2403.14370'>paper</a>
                        <p>We introduce a general framework for generating diverse visual content, including ambiguous images, panorama images, mesh textures, and Gaussian splat textures, by synchronizing multiple diffusion processes.</p>
                    </td>
                </tr>
                <!-- Xu2024Comp4D -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Comp4D: LLM-Guided Compositional 4D Scene Generation</span>
                        <p style="margin:0">Dejia Xu, Hanwen Liang, Neel P. Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos N. Plataniotis, Zhangyang Wang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://vita-group.github.io/Comp4D/'>project page</a> / <a href='https://arxiv.org/pdf/2403.16993.pdf'>paper</a> / <a href='https://github.com/VITA-Group/Comp4D'>code</a> / <a href='https://youtu.be/9q8SV1Xf_Xw'>video</a>
                        <p>Recent advancements in diffusion models for 2D and 3D content creation have sparked a surge of interest in generating 4D content.</p>
                    </td>
                </tr>
                <!-- Lin2024DreamPolisher -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion</span>
                        <p style="margin:0">Yuanze Lin, Ronald Clark, Philip Torr</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://yuanze-lin.me/DreamPolisher_page/'>project page</a> / <a href='https://arxiv.org/pdf/2403.17237'>paper</a> / <a href='https://github.com/yuanze-lin/DreamPolisher'>code</a>
                        <p>We present DreamPolisher, a novel Gaussian Splatting based method with geometric guidance, tailored to learn cross-view consistency and intricate detail from textual descriptions.</p>
                    </td>
                </tr>
                <!-- Chen2023Textto3D -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Text-to-3D using Gaussian Splatting</span>
                        <p style="margin:0">Zilong Chen, Feng Wang, Huaping Liu</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://gsgen3d.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2309.16585.pdf'>paper</a> / <a href='https://github.com/gsgen3d/gsgen'>code</a> / <a href='https://streamable.com/28snte'>video</a>
                        <p>In this paper, we present Gaussian Splatting based text-to-3D generation (GSGEN), a novel approach for generating high-quality 3D objects.</p>
                    </td>
                </tr>
                <!-- Tang2023DreamGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation</span>
                        <p style="margin:0">Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, Gang Zeng</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://dreamgaussian.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2309.16653.pdf'>paper</a> / <a href='https://github.com/dreamgaussian/dreamgaussian'>code</a> / <a href='https://www.youtube.com/live/l956ye13F8M?si=ZkvFL_lsY5OQUB7e'>video</a>
                        <p>Recent advances in 3D content creation mostly leverage optimization-based 3D generation via score distillation sampling (SDS).</p>
                    </td>
                </tr>
                <!-- Yi12023GaussianDreamer -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors</span>
                        <p style="margin:0">Taoran Yi1, Jiemin Fang, Guanjun Wu1, Lingxi Xie, Xiaopeng Zhang,</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://taoranyi.com/gaussiandreamer/'>project page</a> / <a href='https://arxiv.org/pdf/2310.08529.pdf'>paper</a> / <a href='https://github.com/hustvl/GaussianDreamer'>code</a>
                        <p>In recent times, the generation of 3D assets from text prompts has shown impressive results.</p>
                    </td>
                </tr>
                <!-- Li2023GaussianDiffusion -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion Probabilistic Models with Structured Noise</span>
                        <p style="margin:0">Xinhai Li, Huaibin Wang, Kuo-Kun Tseng</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2311.11221.pdf'>paper</a>
                        <p>Text-to-3D, known for its efficient generation methods and expansive creative potential, has garnered significant attention in the AIGC domain.</p>
                    </td>
                </tr>
                <!-- Liang2023LucidDreamer -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching</span>
                        <p style="margin:0">Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, Yingcong Chen</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2311.11284.pdf'>paper</a> / <a href='https://github.com/EnVision-Research/LucidDreamer'>code</a>
                        <p>The recent advancements in text-to-3D generation mark a significant milestone in generative models, unlocking new possibilities for creating imaginative 3D assets across various real-world scenarios.</p>
                    </td>
                </tr>
                <!-- Chung2023LucidDreamer -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes</span>
                        <p style="margin:0">Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, Kyoung Mu Lee</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://luciddreamer-cvlab.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2311.13384.pdf'>paper</a> / <a href='https://github.com/anonymous-luciddreamer/LucidDreamer'>code</a>
                        <p>With the widespread usage of VR devices and contents, demands for 3D scene generation techniques become more popular.</p>
                    </td>
                </tr>
                <!-- Liu2023HumanGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting</span>
                        <p style="margin:0">Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, Ziwei Liu</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://alvinliu0.github.io/projects/HumanGaussian'>project page</a> / <a href='https://arxiv.org/pdf/2311.17061.pdf'>paper</a> / <a href='https://github.com/alvinliu0/HumanGaussian'>code</a> / <a href='https://www.youtube.com/watch?v=S3djzHoqPKY'>video</a>
                        <p>Realistic 3D human generation from text prompts is a desirable yet challenging task.</p>
                    </td>
                </tr>
                <!-- Vilesov2023CG3D -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">CG3D: Compositional Generation for Text-to-3D</span>
                        <p style="margin:0">Alexander Vilesov, Pradyumna Chari, Achuta Kadambi</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://asvilesov.github.io/CG3D/'>project page</a> / <a href='https://arxiv.org/pdf/2311.17907.pdf'>paper</a> / <a href='https://www.youtube.com/watch?v=FMAVeolsE7s'>video</a>
                        <p>With the onset of diffusion-based generative models and their ability to generate text-conditioned images, content generation has received a massive invigoration.</p>
                    </td>
                </tr>
                <!-- Yang2023Learn -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Learn to Optimize Denoising Scores for 3D Generation - A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting</span>
                        <p style="margin:0">Xiaofeng Yang, Yiwen Chen, Cheng Chen, Chi Zhang, Yi Xu, Xulei Yang, Fayao Liu and Guosheng Lin</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://yangxiaofeng.github.io/demo_diffusion_prior/'>project page</a> / <a href='https://arxiv.org/pdf/2312.04820.pdf'>paper</a> / <a href='https://github.com/yangxiaofeng/LODS'>code</a>
                        <p>We propose a unified framework aimed at enhancing the diffusion priors for 3D generation tasks.</p>
                    </td>
                </tr>
                <!-- Blattmann2023Align -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models</span>
                        <p style="margin:0">Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/'>project page</a> / <a href='https://arxiv.org/pdf/2304.08818.pdf'>paper</a>
                        <p>Recent advancements in 3D reconstruction from single images have been driven by the evolution of generative models.</p>
                    </td>
                </tr>
                <!-- Ren2023DreamGaussian4D -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">DreamGaussian4D: Generative 4D Gaussian Splatting</span>
                        <p style="margin:0">Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, Ziwei Liu</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://jiawei-ren.github.io/projects/dreamgaussian4d/'>project page</a> / <a href='https://arxiv.org/pdf/2312.17142.pdf'>paper</a> / <a href='https://github.com/jiawei-ren/dreamgaussian4d'>code</a>
                        <p>Remarkable progress has been made in 4D content generation recently.</p>
                    </td>
                </tr>
                <!-- Yin20234DGen -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency</span>
                        <p style="margin:0">Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, Yunchao Wei</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://vita-group.github.io/4DGen/'>project page</a> / <a href='https://arxiv.org/pdf/2312.17225.pdf'>paper</a> / <a href='https://github.com/VITA-Group/4DGen'>code</a> / <a href='https://www.youtube.com/watch?v=-bXyBKdpQ1o'>video</a>
                        <p>Aided by text-to-image and text-to-video diffusion models, existing 4D content creation pipelines utilize score distillation sampling to optimize entire dynamic 3D scene.</p>
                    </td>
                </tr>
                <!-- Ouyang2023Text2Immersion -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Text2Immersion: Generative Immersive Scene with 3D Gaussian</span>
                        <p style="margin:0">Hao Ouyang, Kathryn Heal, Stephen Lombardi, Tiancheng Sun</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://ken-ouyang.github.io/text2immersion/index.html'>project page</a> / <a href='https://arxiv.org/pdf/2312.09242.pdf'>paper</a>
                        <p>We introduce Text2Immersion, an elegant method for producing high-quality 3D immersive scenes from text prompts.</p>
                    </td>
                </tr>
                <!-- Zhang2023Repaint123 -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable 2D Repainting</span>
                        <p style="margin:0">Junwu Zhang, Zhenyu Tang, Yatian Pang, Xinhua Cheng, Peng Jin, Yida Wei, Munan Ning, Li Yuan</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://pku-yuangroup.github.io/repaint123/'>project page</a> / <a href='https://arxiv.org/pdf/2312.13271.pdf'>paper</a> / <a href='https://github.com/PKU-YuanGroup/repaint123'>code</a>
                        <p>Recent one image to 3D generation methods commonly adopt Score Distillation Sampling (SDS).</p>
                    </td>
                </tr>
                <!-- Duan20244D -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">4D Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes</span>
                        <p style="margin:0">Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2402.03307.pdf'>paper</a>
                        <p>We consider the problem of novel view synthesis (NVS) for dynamic scenes.</p>
                    </td>
                </tr>
                <!-- Gao2024GaussianFlow -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation</span>
                        <p style="margin:0">Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang, Ulrich Neumann</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://zerg-overmind.github.io/GaussianFlow.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2403.12365'>paper</a> / <a href='https://github.com/Zerg-Overmind/GaussianFlow'>code</a> / <a href='https://www.youtube.com/watch?v=0qRcjTw7-YU'>video</a>
                        <p>Creating 4D fields of Gaussian Splatting from images or videos is a challenging task due to its under-constrained nature.</p>
                    </td>
                </tr>
                <!-- Guo2024Motionaware -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Motion-aware 3D Gaussian Splatting for Efficient Dynamic Scene Reconstruction</span>
                        <p style="margin:0">Zhiyang Guo, Wengang Zhou, Li Li, Min Wang, Houqiang Li</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.11447'>paper</a>
                        <p>3D Gaussian Splatting (3DGS) has become an emerging tool for dynamic scene reconstruction.</p>
                    </td>
                </tr>
                <!-- Xiao2024Bridging -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Bridging 3D Gaussian and Mesh for Freeview Video Rendering</span>
                        <p style="margin:0">Yuting Xiao, Xuan Wang, Jiafei Li, Hongrui Cai, Yanbo Fan, Nan Xue, Minghui Yang, Yujun Shen, Shenghua Gao</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.11453'>paper</a>
                        <p>This is only a preview version of GauMesh.</p>
                    </td>
                </tr>
                <!-- Luiten2023Dynamic -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis</span>
                        <p style="margin:0">Jonathon Luiten, Georgios Kopanas, Bastian Leibe, Deva Ramanan</p>
                        <em>3DV '24, 2023</em>
                        <br>
                        <a href='https://dynamic3dgaussians.github.io/'>project page</a> / <a href='https://dynamic3dgaussians.github.io/paper.pdf'>paper</a> / <a href='https://github.com/JonathonLuiten/Dynamic3DGaussians'>code</a> / <a href='https://www.youtube.com/live/hDuy1TgD8I4?si=6oGN0IYnPRxOibpg'>video</a>
                        <p>We present a method that simultaneously addresses the tasks of dynamic scene novel-view synthesis and six degree-of-freedom (6-DOF) tracking of all dense scene elements.</p>
                    </td>
                </tr>
                <!-- Yang2023Deformable -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction</span>
                        <p style="margin:0">Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, Xiaogang Jin</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://ingra14m.github.io/Deformable-Gaussians/'>project page</a> / <a href='https://arxiv.org/pdf/2309.13101.pdf'>paper</a> / <a href='https://github.com/ingra14m/Deformable-3D-Gaussians'>code</a>
                        <p>Implicit neural representation has opened up new avenues for dynamic scene reconstruction and rendering.</p>
                    </td>
                </tr>
                <!-- Wu20234D -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</span>
                        <p style="margin:0">Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Tian Qi, Xinggang Wang</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://guanjunwu.github.io/4dgs/'>project page</a> / <a href='https://arxiv.org/pdf/2310.08528.pdf'>paper</a> / <a href='https://github.com/hustvl/4DGaussians'>code</a>
                        <p>Representing and rendering dynamic scenes has been an important but challenging task.</p>
                    </td>
                </tr>
                <!-- Yang2023Realtime -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting</span>
                        <p style="margin:0">Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, Li Zhang</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2310.10642.pdf'>paper</a> / <a href='https://github.com/fudan-zvg/4d-gaussian-splatting'>code</a>
                        <p>Reconstructing dynamic 3D scenes from 2D images and generating diverse views over time is challenging due to scene complexity and temporal dynamics.</p>
                    </td>
                </tr>
                <!-- Katsumata2023An -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">An Efficient 3D Gaussian Representation for Monocular/Multi-view Dynamic Scenes</span>
                        <p style="margin:0">Kai Katsumata, Duc Minh Vo, Hideki Nakayama</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2311.12897.pdf'>paper</a> / <a href='https://github.com/raven38/EfficientDynamic3DGaussian'>code</a>
                        <p>In novel view synthesis of scenes from multiple input views, 3D Gaussian splatting emerges as a viable alternative to existing radiance field approaches, delivering great visual quality and real-time rendering.</p>
                    </td>
                </tr>
                <!-- Kratimenos2023DynMF -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting</span>
                        <p style="margin:0">Agelos Kratimenos, Jiahui Lei, Kostas Daniilidis</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://agelosk.github.io/dynmf/'>project page</a> / <a href='https://arxiv.org/pdf/2312.00112.pdf'>paper</a> / <a href='https://github.com/agelosk/dynmf'>code</a>
                        <p>Accurately and efficiently modeling dynamic scenes and motions is considered so challenging a task due to temporal dynamics and motion complexity.</p>
                    </td>
                </tr>
                <!-- Shao2023Control4D -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Control4D: Efficient 4D Portrait Editing with Text</span>
                        <p style="margin:0">Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng, Boyao Zhou, Hongwen Zhang, Yebin Liu</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://control4darxiv.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2305.20082.pdf'>paper</a>
                        <p>We introduce Control4D, an innovative framework for editing dynamic 4D portraits using text instructions.</p>
                    </td>
                </tr>
                <!-- Huang2023SCGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes</span>
                        <p style="margin:0">Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, Xiaojuan Qi</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://yihua7.github.io/SC-GS-web/'>project page</a> / <a href='https://yihua7.github.io/SC-GS-web/materials/SC_GS_Arxiv.pdf'>paper</a> / <a href='https://github.com/yihua7/SC-GS'>code</a>
                        <p>Novel view synthesis for dynamic scenes is still a challenging problem in computer vision and graphics.</p>
                    </td>
                </tr>
                <!-- Das2023Neural -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Neural Parametric Gaussians for Monocular Non-Rigid Object Reconstruction</span>
                        <p style="margin:0">Devikalyan Das, Christopher Wewer, Raza Yunus, Eddy Ilg, Jan Eric Lenssen</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2312.01196.pdf'>paper</a>
                        <p>Reconstructing dynamic objects from monocular videos is a severely underconstrained and challenging problem, and recent work has approached it in various directions.</p>
                    </td>
                </tr>
                <!-- Lin2023GaussianFlow -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle</span>
                        <p style="margin:0">Youtian Lin, Zuozhuo Dai, Siyu Zhu, Yao Yao</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://nju-3dv.github.io/projects/Gaussian-Flow'>project page</a> / <a href='https://arxiv.org/pdf/2310.08528.pdf'>paper</a>
                        <p>We introduce Gaussian-Flow, a novel point-based approach for fast dynamic scene reconstruction and real-time rendering from both multi-view and monocular videos.</p>
                    </td>
                </tr>
                <!-- Yu2023CoGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">CoGS: Controllable Gaussian Splatting</span>
                        <p style="margin:0">Heng Yu, Joel Julin, Zoltán Á. Milacski, Koichiro Niinuma, László A. Jeni</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://cogs2023.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2312.05664.pdf'>paper</a>
                        <p>Capturing and re-animating the 3D structure of articulated objects present significant barriers.</p>
                    </td>
                </tr>
                <!-- Liang2023GauFRe -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View Synthesis</span>
                        <p style="margin:0">Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-Phuoc, Douglas Lanman, James Tompkin, Lei Xiao</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://lynl7130.github.io/gaufre/index.html'>project page</a> / <a href='https://arxiv.org/pdf/2312.11458.pdf'>paper</a> / <a href='https://youtu.be/YweWidWO8rI?si=jMssQdIXQV67kwzS'>video</a>
                        <p>We propose a method for dynamic scene reconstruction using deformable 3D Gaussians that is tailored for monocular video.</p>
                    </td>
                </tr>
                <!-- Li2023Spacetime -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis</span>
                        <p style="margin:0">Zhan Li, Zhang Chen, Zhong Li, Yi Xu</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://oppo-us-research.github.io/SpacetimeGaussians-website/'>project page</a> / <a href='https://arxiv.org/pdf/2312.16812.pdf'>paper</a> / <a href='https://github.com/oppo-us-research/SpacetimeGaussians'>code</a> / <a href='https://www.youtube.com/watch?v=YsPPmf-E6Lg'>video</a>
                        <p>Novel view synthesis of dynamic scenes has been an intriguing yet challenging problem.</p>
                    </td>
                </tr>
                <!-- P2023MDSplatting -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly Deformable Scenes</span>
                        <p style="margin:0">Bardienus P. Duisterhof, Zhao Mandi, Yunchao Yao, Jia-Wei Liu, Mike Zheng Shou, Shuran Song, Jeffrey Ichnowski</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://md-splatting.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2312.00583'>paper</a> / <a href='https://github.com/momentum-robotics-lab/md-splatting'>code</a>
                        <p>Accurate 3D tracking in highly deformable scenes with occlusions and shadows can facilitate new applications in robotics, augmented reality, and generative AI.</p>
                    </td>
                </tr>
                <!-- Shaw2023SWAGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SWAGS: Sampling Windows Adaptively for Dynamic 3D Gaussian Splatting</span>
                        <p style="margin:0">Richard Shaw, Jifei Song, Arthur Moreau, Michal Nazarczuk, Sibi Catley-Chandar, Helisa Dhamo, Eduardo Perez-Pellitero</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2312.13308.pdf'>paper</a>
                        <p>Novel view synthesis has shown rapid progress recently, with methods capable of producing evermore photo-realistic results.</p>
                    </td>
                </tr>
                <!-- Sun20233DGStream -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos</span>
                        <p style="margin:0">Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, Wei Xing</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://sjojok.github.io/3dgstream/'>project page</a> / <a href='https://arxiv.org/pdf/2403.01444.pdf'>paper</a> / <a href='https://github.com/SJoJoK/3DGStream'>code</a>
                        <p>Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes from multi-view videos remains a challenging endeavor.</p>
                    </td>
                </tr>
                <!-- Zhuang2024TIPEditor -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts</span>
                        <p style="margin:0">Jingyu Zhuang, Di Kang, Yan-Pei Cao, Guanbin Li, Liang Lin, Ying Shan</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://zjy526223908.github.io/TIP-Editor/'>project page</a> / <a href='https://arxiv.org/pdf/2401.14828.pdf'>paper</a>
                        <p>Text-driven 3D scene editing has gained significant attention owing to its convenience and user-friendliness.</p>
                    </td>
                </tr>
                <!-- Hu2024Segment -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Segment Anything in 3D Gaussians</span>
                        <p style="margin:0">Xu Hu, Yuxi Wang, Lue Fan, Junsong Fan, Junran Peng, Zhen Lei, Qing Li, Zhaoxiang Zhang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://browse.arxiv.org/pdf/2401.17857.pdf'>paper</a>
                        <p>3D Gaussian Splatting has emerged as an alternative 3D representation of Neural Radiance Fields (NeRFs), benefiting from its high-quality rendering results and real-time rendering speed.</p>
                    </td>
                </tr>
                <!-- Palandra2024GSEdit -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian Splatting</span>
                        <p style="margin:0">Francesco Palandra, Andrea Sanchietti, Daniele Baieri, Emanuele Rodolà</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.05154.pdf'>paper</a>
                        <p>We present GSEdit, a pipeline for text-guided 3D object editing based on Gaussian Splatting models.</p>
                    </td>
                </tr>
                <!-- Wu2024GaussCtrl -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing</span>
                        <p style="margin:0">Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, Victor Adrian Prisacariu</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.08733.pdf'>paper</a>
                        <p>We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed by the 3D Gaussian Splatting (3DGS).</p>
                    </td>
                </tr>
                <!-- Wang2024ViewConsistent -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">View-Consistent 3D Editing with Gaussian Splatting</span>
                        <p style="margin:0">Yuxuan Wang, Xuanyu Yi, Zike Wu, Na Zhao, Long Chen, Hanwang Zhang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.11868.pdf'>paper</a>
                        <p>The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing, offering efficient, high-fidelity rendering and enabling precise local manipulations.</p>
                    </td>
                </tr>
                <!-- Guédon2024Gaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering</span>
                        <p style="margin:0">Antoine Guédon, Vincent Lepetit</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://anttwo.github.io/frosting/'>project page</a> / <a href='https://arxiv.org/pdf/2403.14554'>paper</a> / <a href='https://github.com/Anttwo/Frosting'>code</a> / <a href='https://youtu.be/h7LeWq8sG78'>video</a>
                        <p>We propose Gaussian Frosting, a novel mesh-based representation for high-quality rendering and editing of complex 3D effects in real-time.</p>
                    </td>
                </tr>
                <!-- Guo2024Semantic -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian Splatting</span>
                        <p style="margin:0">Jun Guo, Xiaojian Ma, Yue Fan, Huaping Liu, Qing Li</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://semantic-gaussians.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2403.15624'>paper</a>
                        <p>Open-vocabulary 3D scene understanding presents a significant challenge in computer vision, withwide-ranging applications in embodied agents and augmented reality systems.</p>
                    </td>
                </tr>
                <!-- Chen2023GaussianEditor -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting</span>
                        <p style="margin:0">Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, Guosheng Lin</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://buaacyw.github.io/gaussian-editor/'>project page</a> / <a href='https://arxiv.org/pdf/2311.14521.pdf'>paper</a> / <a href='https://github.com/buaacyw/GaussianEditor'>code</a> / <a href='https://youtu.be/TdZIICSFqsU?si=-U4tyOvaAPqIROYn'>video</a>
                        <p>3D editing plays a crucial role in many areas such as gaming and virtual reality.</p>
                    </td>
                </tr>
                <!-- Fang2023GaussianEditor -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions</span>
                        <p style="margin:0">Jiemin Fang, Junjie Wang, Xiaopeng Zhang, Lingxi Xie, Qi Tian</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://gaussianeditor.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2311.16037.pdf'>paper</a> / <a href='https://youtu.be/KWtALsigR3k?si=h6-A44brd5rm3_CM'>video</a>
                        <p>Recently, impressive results have been achieved in 3D scene editing with text instructions based on a 2D diffusion model.</p>
                    </td>
                </tr>
                <!-- Huang2023Pointn -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Point'n Move: Interactive Scene Object Manipulation on Gaussian Splatting Radiance Fields</span>
                        <p style="margin:0">Jiajun Huang, Hongchuan Yu</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2311.16737.pdf'>paper</a>
                        <p>We propose Point'n Move, a method that achieves interactive scene object manipulation with exposed region inpainting.</p>
                    </td>
                </tr>
                <!-- Ye2023Gaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gaussian Grouping: Segment and Edit Anything in 3D Scenes</span>
                        <p style="margin:0">Mingqiao Ye, Martin Danelljan, Fisher Yu, Lei Ke</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2312.00732.pdf'>paper</a> / <a href='https://github.com/lkeab/gaussian-grouping'>code</a>
                        <p>The recent Gaussian Splatting achieves high-quality and real-time novel-view synthesis of the 3D scenes.</p>
                    </td>
                </tr>
                <!-- Cen2023Segment -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Segment Any 3D Gaussians</span>
                        <p style="margin:0">Jiazhong Cen, Jiemin Fang, Chen Yang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://jumpat.github.io/SAGA/'>project page</a> / <a href='https://jumpat.github.io/SAGA/SAGA_paper.pdf'>paper</a> / <a href='https://github.com/Jumpat/SegAnyGAussians'>code</a>
                        <p>Interactive 3D segmentation in radiance fields is an appealing task since its importance in 3D scene understanding and manipulation.</p>
                    </td>
                </tr>
                <!-- Zhou2023Feature -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields</span>
                        <p style="margin:0">Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, Achuta Kadambi</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://feature-3dgs.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2312.03203.pdf'>paper</a> / <a href='https://www.youtube.com/watch?v=YWZiF-WvMN4&t=4s'>video</a>
                        <p>3D scene representations have gained immense popularity in recent years.</p>
                    </td>
                </tr>
                <!-- Lan20232DGuided -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">2D-Guided 3D Gaussian Segmentation</span>
                        <p style="margin:0">Kun Lan, Haoran Li, Haolin Shi, Wenjun Wu, Yong Liao, Lin Wang, Pengyuan Zhou</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2312.16047.pdf'>paper</a>
                        <p>Recently, 3D Gaussian, as an explicit 3D representation method, has demonstrated strong competitiveness over NeRF (Neural Radiance Fields) in terms of expressing complex scenes and training duration.</p>
                    </td>
                </tr>
                <!-- Shi2023Language -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding</span>
                        <p style="margin:0">Jin-Chuan Shi, Miao Wang, Hao-Bin Duan, Shao-Hua Guan</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://buaavrcg.github.io/LEGaussians/'>project page</a> / <a href='https://arxiv.org/pdf/2311.18482.pdf'>paper</a>
                        <p>Open-vocabulary querying in 3D space is challenging but essential for scene understanding tasks such as object localization and segmentation.</p>
                    </td>
                </tr>
                <!-- Qin2023LangSplat -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">LangSplat: 3D Language Gaussian Splatting</span>
                        <p style="margin:0">Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, Hanspeter Pfister</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://langsplat.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2312.16084.pdf'>paper</a> / <a href='https://github.com/minghanqin/LangSplat'>code</a> / <a href='https://www.youtube.com/watch?v=XMlyjsei-Es'>video</a>
                        <p>Human lives in a 3D world and commonly uses natural language to interact with a 3D scene.</p>
                    </td>
                </tr>
                <!-- Zuo2023FMGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding</span>
                        <p style="margin:0">Xingxing Zuo, Pouya Samangouei, Yunwen Zhou, Yan Di, Mingyang Li</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2401.01970.pdf'>paper</a>
                        <p>Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications.</p>
                    </td>
                </tr>
                <!-- Feng2024Gaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting</span>
                        <p style="margin:0">Yutao Feng, Xiang Feng, Yintong Shang, Ying Jiang, Chang Yu, Zeshun Zong, Tianjia Shao, Hongzhi Wu, Kun Zhou, Chenfanfu Jiang, Yin Yang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://amysteriouscat.github.io/GaussianSplashing/'>project page</a> / <a href='https://browse.arxiv.org/pdf/2401.15318.pdf'>paper</a> / <a href='https://www.youtube.com/watch?v=KgaR1ni-Egg&t'>video</a>
                        <p>We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS.</p>
                    </td>
                </tr>
                <!-- Waczyńska2024GaMeS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting</span>
                        <p style="margin:0">Joanna Waczyńska, Piotr Borycki, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2402.01459.pdf'>paper</a> / <a href='https://github.com/waczjoan/gaussian-mesh-splatting'>code</a>
                        <p>In recent years, a range of neural network-based methods for image rendering have been introduced.</p>
                    </td>
                </tr>
                <!-- Gao2024Meshbased -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Mesh-based Gaussian Splatting for Real-time Large-scale Deformation</span>
                        <p style="margin:0">Lin Gao, Jie Yang, Bo-Tao Zhang, Jia-Mu Sun, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2402.04796.pdf'>paper</a>
                        <p>Neural implicit representations, including Neural Distance Fields and Neural Radiance Fields, have demonstrated significant capabilities for reconstructing surfaces with complicated geometry and topology, and generating novel views of a scene.</p>
                    </td>
                </tr>
                <!-- Zhong2024Reconstruction -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians</span>
                        <p style="margin:0">Licheng Zhong, Hong-Xing Yu, Jiajun Wu, Yunzhu Li</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://zlicheng.com/spring_gaus/'>project page</a> / <a href='https://arxiv.org/pdf/2403.09434'>paper</a> / <a href='https://github.com/Colmar-zlicheng/Spring-Gaus'>code</a>
                        <p>Reconstructing and simulating elastic objects from visual observations is crucial for applications in computer vision and robotics.</p>
                    </td>
                </tr>
                <!-- Xu2024TextureGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing</span>
                        <p style="margin:0">Tian-Xing Xu, Wenbo Hu, Yu-Kun Lai, Ying Shan, Song-Hai Zhang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://slothfulxtx.github.io/TexGS/'>project page</a> / <a href='https://arxiv.org/pdf/2403.10050'>paper</a> / <a href='https://github.com/slothfulxtx/Texture-GS'>code</a>
                        <p>3D Gaussian splatting, emerging as a groundbreaking approach, has drawn increasing attention for its capabilities of high-fidelity reconstruction and real-time rendering.</p>
                    </td>
                </tr>
                <!-- Turkulainen2024DNSplatter -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing</span>
                        <p style="margin:0">Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.17822'>paper</a> / <a href='https://github.com/maturk/dn-splatter'>code</a>
                        <p>3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times.</p>
                    </td>
                </tr>
                <!-- Huang20242D -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">2D Gaussian Splatting for Geometrically Accurate Radiance Fields</span>
                        <p style="margin:0">Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, Shenghua Gao</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://surfsplatting.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2403.17888'>paper</a> / <a href='https://www.youtube.com/watch?v=oaHCtB6yiKU'>video</a>
                        <p>3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed without baking.</p>
                    </td>
                </tr>
                <!-- Qiu2024Feature -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing</span>
                        <p style="margin:0">Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://feature-splatting.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2404.01223'>paper</a> / <a href='https://github.com/vuer-ai/feature_splatting'>code</a>
                        <p>Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes.</p>
                    </td>
                </tr>
                <!-- Xie2023PhysGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics</span>
                        <p style="margin:0">Tianyi Xie, Zeshun Zong, Yuxin Qiu, Xuan Li, Yutao Feng, Yin Yang, Chenfanfu Jiang</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://xpandora.github.io/PhysGaussian/'>project page</a> / <a href='https://arxiv.org/pdf/2311.12198.pdf'>paper</a> / <a href='https://github.com/XPandora/PhysGaussian'>code</a> / <a href='https://drive.google.com/file/d/1eh7vxRxer7gfvPhs8jDE56oRjayBc9oe/view'>video</a>
                        <p>We introduce PhysGaussian, a new method that seamlessly integrates physically grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel motion synthesis.</p>
                    </td>
                </tr>
                <!-- Guédon2023SuGaR -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering</span>
                        <p style="margin:0">Antoine Guédon, Vincent Lepetit</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://imagine.enpc.fr/~guedona/sugar/'>project page</a> / <a href='https://arxiv.org/pdf/2311.12775.pdf'>paper</a> / <a href='https://github.com/Anttwo/SuGaR'>code</a> / <a href='https://www.youtube.com/watch?v=MAkFyWfiBQo.&t'>video</a>
                        <p>We propose a method to allow precise and extremely fast mesh extraction from 3D Gaussian Splatting.</p>
                    </td>
                </tr>
                <!-- Chen2023NeuSG -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance</span>
                        <p style="margin:0">Hanlin Chen, Chen Li, Gim Hee Lee</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2312.00846.pdf'>paper</a>
                        <p>Existing neural implicit surface reconstruction methods have achieved impressive performance in multi-view 3D reconstruction by leveraging explicit geometry priors such as depth maps or point clouds as regularization.</p>
                    </td>
                </tr>
                <!-- Minh2024Characterizing -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting</span>
                        <p style="margin:0">Van Minh Nguyen, Emma Sandidge, Trupti Mahendrakar, Ryan T. White</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2401.02588.pdf'>paper</a>
                        <p>The accelerating deployment of spacecraft in orbit have generated interest in on-orbit servicing (OOS), inspection of spacecraft, and active debris removal (ADR).</p>
                    </td>
                </tr>
                <!-- Franke2024TRIPS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering</span>
                        <p style="margin:0">Linus Franke, Darius Rückert, Laura Fink, Marc Stamminger</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://lfranke.github.io/trips/'>project page</a> / <a href='https://arxiv.org/pdf/2401.06003.pdf'>paper</a> / <a href='https://github.com/lfranke/trips'>code</a>
                        <p>Point-based radiance field rendering has demonstrated impressive results for novel view synthesis, offering a compelling blend of rendering quality and computational efficiency.</p>
                    </td>
                </tr>
                <!-- Zhu2024EndoGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">EndoGS: Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</span>
                        <p style="margin:0">Lingting Zhu, Zhao Wang, Jiahao Cui, Zhenchao Jin, Guying Lin, Lequan Yu</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2401.11535.pdf'>paper</a> / <a href='https://github.com/HKU-MedAI/EndoGS'>code</a>
                        <p>Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos.</p>
                    </td>
                </tr>
                <!-- Liu2024EndoGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">EndoGaussian: Gaussian Splatting for Deformable Surgical Scene Reconstruction</span>
                        <p style="margin:0">Yifan Liu, Chenxin Li, Chen Yang, Yixuan Yuan</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://yifliu3.github.io/EndoGaussian/'>project page</a> / <a href='https://arxiv.org/pdf/2401.12561.pdf'>paper</a> / <a href='https://github.com/yifliu3/EndoGaussian'>code</a>
                        <p>Reconstructing deformable tissues from endoscopic stereo videos is essential in many downstream surgical applications.</p>
                    </td>
                </tr>
                <!-- Xiong2024GauUScene -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting</span>
                        <p style="margin:0">Butian Xiong, Zhuo Li, Zhen Li</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2401.14032.pdf'>paper</a>
                        <p>We introduce a novel large-scale scene reconstruction benchmark using the newly developed 3D representation approach, Gaussian Splatting, on our expansive U-Scene dataset.</p>
                    </td>
                </tr>
                <!-- Hong2024LIVGaussMap -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering</span>
                        <p style="margin:0">Sheng Hong, Junjie He, Xinhu Zheng, Hesheng Wang, Hao Fang, Kangcheng Liu, Chunran Zheng, Shaojie Shen</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2401.14857.pdf'>paper</a> / <a href='https://github.com/sheng00125/LIV-GaussMap'>code</a>
                        <p>We introduce an integrated precise LiDAR, Inertial, and Visual (LIV) multi-modal sensor fused mapping system that builds on the differentiable surface splatting to improve the mapping fidelity, quality, and structural accuracy.</p>
                    </td>
                </tr>
                <!-- Jiang2024VRGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality</span>
                        <p style="margin:0">Ying Jiang, Chang Yu, Tianyi Xie, Xuan Li, Yutao Feng, Huamin Wang, Minchen Li, Henry Lau, Feng Gao, Yin Yang, Chenfanfu Jiang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://yingjiang96.github.io/VR-GS/'>project page</a> / <a href='https://arxiv.org/pdf/2401.16663.pdf'>paper</a>
                        <p>As consumer Virtual Reality (VR) and Mixed Reality (MR) technologies gain momentum, there's a growing focus on the development of engagements with 3D virtual content.</p>
                    </td>
                </tr>
                <!-- Chen2024SplatNav -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps</span>
                        <p style="margin:0">Timothy Chen, Ola Shorinwa, Weijia Zeng, Joseph Bruno, Philip Dames, Mac Schwager</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.02751.pdf'>paper</a>
                        <p>We present Splat-Nav, a navigation pipeline that consists of a real-time safe planning module and a robust state estimation module designed to operate in the Gaussian Splatting (GSplat) environment representation, a popular emerging 3D scene representation from computer vision.</p>
                    </td>
                </tr>
                <!-- Cai2024Radiative -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis</span>
                        <p style="margin:0">TYuanhao Cai, Yixun Liang, Jiahao Wang, Angtian Wang, Yulun Zhang, Xiaokang Yang, Zongwei Zhou, Alan Yuille</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.04116.pdf'>paper</a>
                        <p>X-ray is widely applied for transmission imaging due to its stronger penetration than natural light.</p>
                    </td>
                </tr>
                <!-- Lu2024ManiGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation</span>
                        <p style="margin:0">Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://guanxinglu.github.io/ManiGaussian/'>project page</a> / <a href='https://arxiv.org/pdf/2403.08321.pdf'>paper</a> / <a href='https://github.com/GuanxingLu/ManiGaussian'>code</a>
                        <p>Performing language-conditioned robotic manipulation tasks in unstructured environments is highly demanded for general intelligent robots.</p>
                    </td>
                </tr>
                <!-- Zhang2024GaussianImage -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting</span>
                        <p style="margin:0">Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei Qin, Guo Lu, Jing Geng, Jun Zhang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.08551'>paper</a>
                        <p>Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available.</p>
                    </td>
                </tr>
                <!-- Zheng2024GaussianGrasper -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping</span>
                        <p style="margin:0">Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu, Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zengmao Wang, Lina Liu, Chao Yang, Dawei Wang, Zhen Chen, Xiaoxiao Long, Meiqing Wang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.09637'>paper</a> / <a href='https://github.com/MrSecant/GaussianGrasper'>code</a>
                        <p>Constructing a 3D scene capable of accommodating open-ended language queries, is a pivotal pursuit, particularly within the domain of robotics.</p>
                    </td>
                </tr>
                <!-- Yu2024DenSOFT -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Den-SOFT: Dense Space-Oriented Light Field DataseT for 6-DOF Immersive Experience</span>
                        <p style="margin:0">Xiaohang Yu, Zhengxian Yang, Shi Pan, Yuqi Han, Haoxiang Wang, Jun Zhang, Shi Yan, Borong Lin, Lei Yang, Tao Yu, Lu Fang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.09973.pdf'>paper</a>
                        <p>We have built a custom mobile multi-camera large-space dense light field capture system, which provides a series of high-quality and sufficiently dense light field images for various scenarios.</p>
                    </td>
                </tr>
                <!-- Savant2024Modeling -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Modeling uncertainty for Gaussian Splatting</span>
                        <p style="margin:0">Luca Savant, Diego Valsesia, Enrico Magli</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.18476'>paper</a>
                        <p>We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS).</p>
                    </td>
                </tr>
                <!-- Zhang2024TOGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D DSA Rendering</span>
                        <p style="margin:0">Shuai Zhang, Huangxuan Zhao, Zhenghong Zhou, Guanjun Wu, Chuansheng Zheng, Xinggang Wang, Wenyu Liu</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.19586'>paper</a>
                        <p>Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical imaging technique that provides a series of 2D images captured at different stages and angles during the process of contrast agent filling blood vessels.</p>
                    </td>
                </tr>
                <!-- Jiang2023FisherRF -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">FisherRF: Active View Selection and Uncertainty Quantification for Radiance Fields using Fisher Information</span>
                        <p style="margin:0">Wen Jiang, Boshu Lei, Kostas Daniilidis</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://jiangwenpl.github.io/FisherRF/'>project page</a> / <a href='https://arxiv.org/pdf/2311.17874.pdf'>paper</a> / <a href='https://github.com/JiangWenPL/FisherRF'>code</a>
                        <p>This study addresses the challenging problem of active view selection and uncertainty quantification within the domain of Radiance Fields.</p>
                    </td>
                </tr>
                <!-- Chen2023Periodic -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time Rendering</span>
                        <p style="margin:0">Yurui Chen, Chun Gu, Junzhe Jiang, Xiatian Zhu, Li Zhang</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://fudan-zvg.github.io/PVG/'>project page</a> / <a href='https://arxiv.org/pdf/2311.18561.pdf'>paper</a> / <a href='https://github.com/fudan-zvg/PVG'>code</a>
                        <p>Modeling dynamic, large-scale urban scenes is challenging due to their highly intricate geometric structures and unconstrained dynamics in both space and time.</p>
                    </td>
                </tr>
                <!-- Pokhariya2023MANUS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">MANUS: Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians</span>
                        <p style="margin:0">Chandradeep Pokhariya, Ishaan N Shah, Angela Xing, Zekun Li, Kefan Chen, Avinash Sharma, Srinath Sridhar</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2312.02137.pdf'>paper</a>
                        <p>Understanding how we grasp objects with our hands has important applications in areas like robotics and mixed reality.</p>
                    </td>
                </tr>
                <!-- Zou2023Triplane -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers</span>
                        <p style="margin:0">Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, Song-Hai Zhang</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://zouzx.github.io/TriplaneGaussian/'>project page</a> / <a href='https://arxiv.org/pdf/2312.09147.pdf'>paper</a> / <a href='https://github.com/VAST-AI-Research/TriplaneGaussian'>code</a>
                        <p>Recent advancements in 3D reconstruction from single images have been driven by the evolution of generative models.</p>
                    </td>
                </tr>
                <!-- Ye2023Mathematical -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Mathematical Supplement for the gsplat Library</span>
                        <p style="margin:0">Vickie Ye, Angjoo Kanazawa</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2312.02121.pdf'>paper</a>
                        <p>This report provides the mathematical details of the gsplat library, a modular toolbox for efficient differentiable Gaussian splatting, as proposed by Kerbl et al.</p>
                    </td>
                </tr>
                <!-- Meyer2023PEGASUS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for 6DOF Object Pose Dataset Generation</span>
                        <p style="margin:0">Lukas Meyer, Floris Erich, Yusuke Yoshiyasu, Marc Stamminger, Noriaki Ando, Yukiyasu Domae</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://meyerls.github.io/pegasus_web/'>project page</a> / <a href='https://arxiv.org/pdf/2401.02281.pdf'>paper</a> / <a href='https://github.com/meyerls/PEGASUS'>code</a>
                        <p>Modeling dynamic, large-scale urban scenes is challenging due to their highly intricate geometric structures and unconstrained dynamics in both space and time.</p>
                    </td>
                </tr>
                <!-- Durvasula2024DISTWAR -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">DISTWAR: Fast Differentiable Rendering on Raster-based Rendering Pipelines</span>
                        <p style="margin:0">Sankeerth Durvasula, Adrian Zhao, Fan Chen, Ruofan Liang, Pawan Kumar Sanjaya, Nandita Vijaykumar</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2401.05345.pdf'>paper</a>
                        <p>Differentiable rendering is a technique used in an important emerging class of visual computing applications that involves representing a 3D scene as a model that is trained from 2D images using gradient descent.</p>
                    </td>
                </tr>
                <!-- Zhang2024FreGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization</span>
                        <p style="margin:0">Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, Eric Xing</p>
                        <em>CVPR '24, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.06908.pdf'>paper</a>
                        <p>3D Gaussian splatting has achieved very impressive performance in real-time novel view synthesis.</p>
                    </td>
                </tr>
                <!-- Jung2024RAINGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">RAIN-GS: Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting</span>
                        <p style="margin:0">Jaewoo Jung, Jisang Han, Honggyu An, Jiwon Kang, Seonghoon Park, Seungryong Kim</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://ku-cvlab.github.io/RAIN-GS/'>project page</a> / <a href='https://arxiv.org/pdf/2403.09413'>paper</a> / <a href='https://github.com/KU-CVLAB/RAIN-GS'>code</a>
                        <p>3D Gaussian splatting (3DGS) has recently demonstrated impressive capabilities in real-time novel view synthesis and 3D reconstruction.</p>
                    </td>
                </tr>
                <!-- Feng2024A -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">A New Split Algorithm for 3D Gaussian Splatting</span>
                        <p style="margin:0">Qiyuan Feng, Gengchen Cao, Haoxiang Chen, Tai-Jiang Mu, Ralph R. Martin, Shi-Min Hu</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.09143'>paper</a>
                        <p>3D Gaussian splatting models, as a novel explicit 3D representation, have been applied in many domains recently, such as explicit geometric editing and geometry generation.</p>
                    </td>
                </tr>
                <!-- Chung2023DepthRegularized -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images</span>
                        <p style="margin:0">Jaeyoung Chung, Jeongtaek Oh, Kyoung Mu Lee</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://robot0321.github.io/DepthRegGS/index.html'>project page</a> / <a href='https://arxiv.org/pdf/2311.13398.pdf'>paper</a> / <a href='https://github.com/robot0321/DepthRegularizedGS'>code</a>
                        <p>In this paper, we present a method to optimize Gaussian splatting with a limited number of images while avoiding overfitting.</p>
                    </td>
                </tr>
                <!-- Girish2023EAGLES -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS</span>
                        <p style="margin:0">Sharath Girish, Kamal Gupta, Abhinav Shrivastava</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://efficientgaussian.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2312.04564.pdf'>paper</a> / <a href='https://github.com/Sharath-girish/efficientgaussian'>code</a>
                        <p>Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view scene synthesis.</p>
                    </td>
                </tr>
                <!-- Fu2023COLMAPFree -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">COLMAP-Free 3D Gaussian Splatting</span>
                        <p style="margin:0">Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A. Efros, Xiaolong Wang</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://oasisyang.github.io/colmap-free-3dgs/'>project page</a> / <a href='https://arxiv.org/pdf/2312.07504.pdf'>paper</a> / <a href='https://youtu.be/IJtnx4keJvg'>video</a>
                        <p>While neural rendering has led to impressive advances in scene reconstruction and novel view synthesis, it relies heavily on accurately pre-computed camera poses.</p>
                    </td>
                </tr>
                <!-- Sun2023iComMa -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">iComMa: Inverting 3D Gaussians Splatting for Camera Pose Estimation via Comparing and Matching</span>
                        <p style="margin:0">Yuan Sun, Xuan Wang, Yunfan Zhang, Jie Zhang, Caigui Jiang, Yu Guo, Fei Wang</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2312.09031.pdf'>paper</a>
                        <p>We present a method named iComMa to address the 6D pose estimation problem in computer vision.</p>
                    </td>
                </tr>
                <!-- Bolanos2024Gaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gaussian Shadow Casting for Neural Characters</span>
                        <p style="margin:0">Luis Bolanos, Shih-Yang Su, Helge Rhodin</p>
                        <em>CVPR '24, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2401.06116.pdf'>paper</a>
                        <p>Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting.</p>
                    </td>
                </tr>
                <!-- Huang2024Optimal -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Optimal Projection for 3D Gaussian Splatting</span>
                        <p style="margin:0">Letian Huang, Jiayang Bai, Jie Guo, Yanwen Guo</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://browse.arxiv.org/pdf/2402.00752.pdf'>paper</a>
                        <p>3D Gaussian Splatting has garnered extensive attention and application in real-time neural rendering.</p>
                    </td>
                </tr>
                <!-- Huang2024360GS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming</span>
                        <p style="margin:0">Letian Huang, Jiayang Bai, Jie Guo, Yanwen Guo</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://browse.arxiv.org/pdf/2402.00763.pdf'>paper</a>
                        <p>3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings.</p>
                    </td>
                </tr>
                <!-- Radl2024StopThePop -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time Rendering</span>
                        <p style="margin:0">Lukas Radl, Michael Steiner, Mathias Parger, Alexander Weinrauch, Bernhard Kerbl, Markus Steinberger</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='arxiv.org/pdf/2402.00525.pdf'>paper</a> / <a href='https://youtu.be/RJQlSORNkr0'>video</a>
                        <p>Gaussian Splatting has emerged as a prominent model for constructing 3D representations from images across diverse domains.</p>
                    </td>
                </tr>
                <!-- Hamdi2024GES -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering</span>
                        <p style="margin:0">Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, Andrea Vedaldi</p>
                        <em>CVPR '24, 2024</em>
                        <br>
                        <a href='https://abdullahamdi.com/ges/'>project page</a> / <a href='https://arxiv.org/pdf/2402.10128.pdf'>paper</a> / <a href='https://github.com/ajhamdi/ges-splatting'>code</a> / <a href='https://youtu.be/edSvNy3roV8?si=VGncH7op1OfqkEtx'>video</a>
                        <p>Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation.</p>
                    </td>
                </tr>
                <!-- Jo2024Identifying -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting</span>
                        <p style="margin:0">Joongho Jo, Hyeongwon Kim, Jongsun Park</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2402.13827.pdf'>paper</a>
                        <p>3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality.</p>
                    </td>
                </tr>
                <!-- Cheng2024GaussianPro -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianPro: 3D Gaussian Splatting with Progressive Propagation</span>
                        <p style="margin:0">Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin, Yuexin Ma, Wenping Wang, Xuejin Chen</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://kcheng1021.github.io/gaussianpro.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2402.14650.pdf'>paper</a> / <a href='https://github.com/kcheng1021/GaussianPro'>code</a>
                        <p>The advent of 3D Gaussian Splatting (3DGS) has recently brought about a revolution in the field of neural rendering, facilitating high-quality renderings at real-time speed.</p>
                    </td>
                </tr>
                <!-- Yang2024SpecGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting</span>
                        <p style="margin:0">Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, Xiaogang Jin</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2402.15870.pdf'>paper</a>
                        <p>The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality.</p>
                    </td>
                </tr>
                <!-- Lin2024VastGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction</span>
                        <p style="margin:0">Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, Wenming Yang</p>
                        <em>CVPR '24, 2024</em>
                        <br>
                        <a href='https://vastgaussian.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2402.17427.pdf'>paper</a>
                        <p>Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed.</p>
                    </td>
                </tr>
                <!-- Eric20243D -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">3D Gaussian Model for Animation and Texturing</span>
                        <p style="margin:0">Xiangzhi Eric Wang, Zackary P. T. Sin</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2402.19441.pdf'>paper</a>
                        <p>3D Gaussian Splatting has made a marked impact on neural rendering by achieving impressive fidelity and performance.</p>
                    </td>
                </tr>
                <!-- Peng2024BAGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling</span>
                        <p style="margin:0">Cheng Peng, Yutao Tang, Yifan Zhou, Nengyu Wang, Xijun Liu, Deming Li, Rama Chellappa</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.04926.pdf'>paper</a>
                        <p>Recent efforts in using 3D Gaussians for scene reconstruction and novel view synthesis can achieve impressive results on curated benchmarks; however, images captured in real life are often blurry.</p>
                    </td>
                </tr>
                <!-- Liu2024StyleGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting</span>
                        <p style="margin:0">Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, Shijian Lu</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://kunhao-liu.github.io/StyleGaussian/'>project page</a> / <a href='https://arxiv.org/pdf/2403.04926.pdf'>paper</a> / <a href='https://github.com/Kunhao-Liu/StyleGaussian'>code</a>
                        <p>We introduce StyleGaussian, a novel 3D style transfer technique that allows instant transfer of any image's style to a 3D scene at 10 frames per second (fps).</p>
                    </td>
                </tr>
                <!-- Saroha2024Gaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gaussian Splatting in Style</span>
                        <p style="margin:0">Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel Cremers</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.08498'>paper</a>
                        <p>Scene stylization extends the work of neural style transfer to three spatial dimensions.</p>
                    </td>
                </tr>
                <!-- Zhao2024BADGaussians -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting</span>
                        <p style="margin:0">Lingzhe Zhao, Peng Wang, Peidong Liu</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://lingzhezhao.github.io/BAD-Gaussians/'>project page</a> / <a href='https://arxiv.org/pdf/2403.11831.pdf'>paper</a> / <a href='https://github.com/WU-CVGL/BAD-Gaussians/'>code</a>
                        <p>While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses.</p>
                    </td>
                </tr>
                <!-- Dahmani2024SWAG -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians</span>
                        <p style="margin:0">Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.10427.pdf'>paper</a>
                        <p>Implicit neural representation methods have shown impressive advancements in learning 3D scenes from unstructured in-the-wild photo collections but are still limited by the large computational cost of volumetric rendering.</p>
                    </td>
                </tr>
                <!-- Li2024GeoGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering</span>
                        <p style="margin:0">Yanyan Li, Chenyu Lyu, Yan Di, Guangyao Zhai, Gim Hee Lee, Federico Tombari</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.11324.pdf'>paper</a>
                        <p>During the Gaussian Splatting optimization process, the scene's geometry can gradually deteriorate if its structure is not deliberately preserved, especially in non-textured regions such as walls, ceilings, and furniture surfaces.</p>
                    </td>
                </tr>
                <!-- Liang2024AnalyticSplatting -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Analytic-Splatting: Anti-Aliased 3D Gaussian Splatting via Analytic Integration</span>
                        <p style="margin:0">Zhihao Liang, Qi Zhang, Wenbo Hu, Ying Feng, Lei Zhu, Kui Jia</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.11056.pdf'>paper</a>
                        <p>The 3D Gaussian Splatting (3DGS) gained its popularity recently by combining the advantages of both primitive-based and volumetric 3D representations, resulting in improved quality and efficiency for 3D scene rendering.</p>
                    </td>
                </tr>
                <!-- Seiskari2024Gaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation for Natural Camera Motion</span>
                        <p style="margin:0">Otto Seiskari, Jerry Ylilammi, Valtteri Kaatrasalo, Pekka Rantalankila, Matias Turkulainen, Juho Kannala, Esa Rahtu, Arno Solin</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://spectacularai.github.io/3dgs-deblur/'>project page</a> / <a href='https://arxiv.org/pdf/2403.13327.pdf'>paper</a> / <a href='https://github.com/SpectacularAI/3dgs-deblur'>code</a>
                        <p>High-quality scene reconstruction and novel view synthesis based on Gaussian Splatting (3DGS) typically require steady, high-quality photographs, often impractical to capture with handheld cameras.</p>
                    </td>
                </tr>
                <!-- Niemeyer2024RadSplat -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS</span>
                        <p style="margin:0">Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, Federico Tombari</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://m-niemeyer.github.io/radsplat/'>project page</a> / <a href='https://arxiv.org/pdf/2403.13806.pdf'>paper</a>
                        <p>Recent advances in view synthesis and real-time rendering have achieved photorealistic quality at impressive rendering speeds.</p>
                    </td>
                </tr>
                <!-- Fang2024MiniSplatting -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians</span>
                        <p style="margin:0">Guangchi Fang, Bing Wang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.14166.pdf'>paper</a>
                        <p>In this study, we explore the challenge of efficiently representing scenes with a constrained number of Gaussians.</p>
                    </td>
                </tr>
                <!-- Zhang2024PixelGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting</span>
                        <p style="margin:0">Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, Hengshuang Zhao</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.15530.pdf'>paper</a>
                        <p>3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results while advancing real-time rendering performance.</p>
                    </td>
                </tr>
                <!-- Zhang2024Gaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gaussian in the Wild: 3D Gaussian Splatting for Unconstrained Image Collections</span>
                        <p style="margin:0">Dongbin Zhang, Chuming Wang, Weitao Wang, Peihao Li, Minghan Qin, Haoqian Wang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://eastbeanzhang.github.io/GS-W/'>project page</a> / <a href='https://arxiv.org/pdf/2403.15704'>paper</a> / <a href='https://github.com/EastbeanZhang/Gaussian-Wild'>code</a> / <a href='https://www.youtube.com/watch?v=BNIX-OmIzgo'>video</a>
                        <p>Novel view synthesis from unconstrained in-the-wild images remains a meaningful but challenging task.</p>
                    </td>
                </tr>
                <!-- Yu2024GSDF -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction</span>
                        <p style="margin:0">Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, Bo Dai</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://city-super.github.io/GSDF/'>project page</a> / <a href='https://arxiv.org/pdf/2403.16964'>paper</a> / <a href='https://github.com/city-super/GSDF'>code</a>
                        <p>Presenting a 3D scene from multiview images remains a core and long-standing challenge in computer vision and computer graphics.</p>
                    </td>
                </tr>
                <!-- Ren2024OctreeGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians</span>
                        <p style="margin:0">Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, Bo Dai</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://city-super.github.io/octree-gs/'>project page</a> / <a href='https://arxiv.org/pdf/2403.17898'>paper</a> / <a href='https://github.com/city-super/Octree-GS'>code</a>
                        <p>The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering fidelity and efficiency compared to NeRF-based neural scene representations.</p>
                    </td>
                </tr>
                <!-- Song2024SAGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing</span>
                        <p style="margin:0">Xiaowei Song, Jv Zheng, Shiran Yuan, Huan-ang Gao, Jingwei Zhao, Xiang He, Weihao Gu, Hao Zhao</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://kevinsong729.github.io/project-pages/SA-GS/'>project page</a> / <a href='https://arxiv.org/pdf/2403.19615'>paper</a> / <a href='https://github.com/zsy1987/SA-GS/'>code</a>
                        <p>In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian Splatting (SA-GS).</p>
                    </td>
                </tr>
                <!-- Comi2024Snapit -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces</span>
                        <p style="margin:0">Mauro Comi, Alessio Tonioni, Max Yang, Jonathan Tremblay, Valts Blukis, Yijiong Lin, Nathan F. Lepora, Laurence Aitchison</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.20275'>paper</a>
                        <p>Touch and vision go hand in hand, mutually enhancing our ability to understand the world.</p>
                    </td>
                </tr>
                <!-- Comi2024Snapit -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces</span>
                        <p style="margin:0">Mauro Comi, Alessio Tonioni, Max Yang, Jonathan Tremblay, Valts Blukis, Yijiong Lin, Nathan F. Lepora, Laurence Aitchison</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2404.00409.pdf'>paper</a> / <a href='https://github.com/CVMI-Lab/3DGSR'>code</a>
                        <p>In this paper, we present an implicit surface reconstruction method with 3D Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D reconstruction with intricate details while inheriting the high efficiency and rendering quality of 3DGS.</p>
                    </td>
                </tr>
                <!-- Meng2024Mirror3DGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</span>
                        <p style="margin:0">Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2404.01168.pdf'>paper</a>
                        <p>3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis.</p>
                    </td>
                </tr>
                <!-- Yu2023MipSplatting -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Mip-Splatting Alias-free 3D Gaussian Splatting</span>
                        <p style="margin:0">Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, Andreas Geiger</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://niujinshuchong.github.io/mip-splatting/'>project page</a> / <a href='https://arxiv.org/pdf/2311.16493.pdf'>paper</a> / <a href='https://github.com/autonomousvision/mip-splatting'>code</a>
                        <p>Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, reaching high fidelity and efficiency.</p>
                    </td>
                </tr>
                <!-- Gao2023Relightable -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF Decomposition and Ray Tracing</span>
                        <p style="margin:0">Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, Yao Yao</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://nju-3dv.github.io/projects/Relightable3DGaussian/'>project page</a> / <a href='https://arxiv.org/pdf/2311.16043.pdf'>paper</a> / <a href='https://github.com/NJU-3DV/Relightable3DGaussian'>code</a>
                        <p>We present a novel differentiable point-based rendering framework for material and lighting decomposition from multi-view images, enabling editing, ray-tracing, and real-time relighting of the 3D point cloud.</p>
                    </td>
                </tr>
                <!-- Liang2023GSIR -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GS-IR: 3D Gaussian Splatting for Inverse Rendering</span>
                        <p style="margin:0">Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, Kui Jia</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://github.com/lzhnb/GS-IR'>project page</a> / <a href='https://arxiv.org/pdf/2311.16473.pdf'>paper</a> / <a href='https://github.com/lzhnb/GS-IR'>code</a>
                        <p>We propose GS-IR, a novel inverse rendering approach based on 3D Gaussian Splatting (GS) that leverages forward mapping volume rendering to achieve photorealistic novel view synthesis and relighting results.</p>
                    </td>
                </tr>
                <!-- Yan2023MultiScale -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering</span>
                        <p style="margin:0">Zhiwen Yan, Weng Fei Low, Yu Chen, Gim Hee Lee</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2311.17089.pdf'>paper</a>
                        <p>3D Gaussians have recently emerged as a highly efficient representation for 3D reconstruction and rendering.</p>
                    </td>
                </tr>
                <!-- Jiang2023GaussianShader -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective Surfaces</span>
                        <p style="margin:0">Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaoxiao Long, Wenping Wang, Yuexin Ma</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://asparagus15.github.io/GaussianShader.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2311.17977.pdf'>paper</a> / <a href='https://github.com/Asparagus15/GaussianShader'>code</a>
                        <p>The advent of neural 3D Gaussians has recently brought about a revolution in the field of neural rendering, facilitating the generation of high-quality renderings at real-time speeds.</p>
                    </td>
                </tr>
                <!-- Lu2023ScaffoldGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering</span>
                        <p style="margin:0">Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, Bo Dai</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://city-super.github.io/scaffold-gs/'>project page</a> / <a href='https://arxiv.org/pdf/2312.00109.pdf'>paper</a> / <a href='https://github.com/city-super/Scaffold-GS'>code</a>
                        <p>Neural rendering methods have significantly advanced photo-realistic 3D scene rendering in various academic and industrial applications.</p>
                    </td>
                </tr>
                <!-- Lee2023Deblurring -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Deblurring 3D Gaussian Splatting</span>
                        <p style="margin:0">Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/'>project page</a> / <a href='https://arxiv.org/pdf/2401.00834.pdf'>paper</a> / <a href='https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting'>code</a>
                        <p>Recent studies in Radiance Fields have paved the robust way for novel view synthesis with their photorealistic rendering quality.</p>
                    </td>
                </tr>
                <!-- Shi2023GIR -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GIR: 3D Gaussian Inverse Rendering for Relightable Scene Factorization</span>
                        <p style="margin:0">Yahao Shi, Yanmin Wu, Chenming Wu, Xing Liu, Chen Zhao, Haocheng Feng, Jingtuo Liu, Liangjun Zhang, Jian Zhang, Bin Zhou, Errui Ding, Jingdong Wang</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://3dgir.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2312.05133'>paper</a>
                        <p>This paper presents GIR, a 3D Gaussian Inverse Rendering method for relightable scene factorization.</p>
                    </td>
                </tr>
                <!-- Malarz2023Gaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gaussian Splatting with NeRF-based Color and Opacity</span>
                        <p style="margin:0">Dawid Malarz, Weronika Smolak, Jacek Tabor, Sławomir Tadeja, Przemysław Spurek</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2312.13729.pdf'>paper</a> / <a href='https://github.com/gmum/ViewingDirectionGaussianSplatting'>code</a>
                        <p>Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of neural networks to capture the intricacies of 3D objects.</p>
                    </td>
                </tr>
                <!-- Bai2024Progress -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human</span>
                        <p style="margin:0">Song Bai, Jie Li</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2401.02620.pdf'>paper</a>
                        <p>While AI-generated text and 2D images continue to expand its territory, 3D generation has gradually emerged as a trend that cannot be ignored.</p>
                    </td>
                </tr>
                <!-- Chen2024A -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">A Survey on 3D Gaussian Splatting</span>
                        <p style="margin:0">Guikun Chen, Wenguan Wang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2401.03890.pdf'>paper</a>
                        <p>3D Gaussian splatting (3D GS) has recently emerged as a transformative technique in the explicit radiance field and computer graphics landscape.</p>
                    </td>
                </tr>
                <!-- Fei20243D -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">3D Gaussian as a New Vision Era: A Survey</span>
                        <p style="margin:0">Ben Fei, Jingyi Xu, Rui Zhang, Qingyuan Zhou, Weidong Yang, Ying He</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2402.07181.pdf'>paper</a>
                        <p>3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the field of Computer Graphics, offering explicit scene representation and novel view synthesis without the reliance on neural networks, such as Neural Radiance Fields (NeRF).</p>
                    </td>
                </tr>
                <!-- Tosi2024How -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey</span>
                        <p style="margin:0">Fabio Tosi, Youmin Zhang, Ziren Gong, Erik Sandström, Stefano Mattoccia, Martin R. Oswald, Matteo Poggi</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2402.13255.pdf'>paper</a>
                        <p>Over the past two decades, research in the field of Simultaneous Localization and Mapping (SLAM) has undergone a significant evolution, highlighting its critical role in enabling autonomous exploration of unknown environments.</p>
                    </td>
                </tr>
                <!-- Wu2024Recent -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Recent Advances in 3D Gaussian Splatting</span>
                        <p style="margin:0">Tong Wu, Yu-Jie Yuan, Ling-Xiao Zhang, Jie Yang, Yan-Pei Cao, Ling-Qi Yan, Lin Gao</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.11134.pdf'>paper</a>
                        <p>The emergence of 3D Gaussian Splatting (3DGS) has greatly accelerated the rendering speed of novel view synthesis.</p>
                    </td>
                </tr>
                <!-- Li2024SGSSLAM -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</span>
                        <p style="margin:0">Mingrui Li, Shuhong Liu, Heng Zhou</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2402.03246.pdf'>paper</a>
                        <p>Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation.</p>
                    </td>
                </tr>
                <!-- Zhu2024SemGaussSLAM -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM</span>
                        <p style="margin:0">Siting Zhu, Renjie Qin, Guangming Wang, Jiuming Liu, Hesheng Wang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.07494.pdf'>paper</a>
                        <p>We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3D Gaussian representation, that enables accurate 3D semantic mapping, robust camera tracking, and high-quality rendering in real-time.</p>
                    </td>
                </tr>
                <!-- Deng2024Compact -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Compact 3D Gaussian Splatting For Dense Visual SLAM</span>
                        <p style="margin:0">Tianchen Deng, Yaohui Chen, Leyan Zhang, Jianfei Yang, Shenghai Yuan, Danwei Wang, Weidong Chen</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.11247.pdf'>paper</a>
                        <p>Recent work has shown that 3D Gaussian-based SLAM enables high-quality reconstruction, accurate pose estimation, and real-time rendering of scenes.</p>
                    </td>
                </tr>
                <!-- Ji2024NEDSSLAM -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting</span>
                        <p style="margin:0">Yiming Ji, Yang Liu, Guanghu Xie, Boyu Ma, Zongwu Xie</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.11679.pdf'>paper</a>
                        <p>We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D Gaussian representation, that enables robust 3D semantic mapping, accurate camera tracking, and high-quality rendering in real-time.</p>
                    </td>
                </tr>
                <!-- Sun2024HighFidelity -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided Densification and Regularized Optimization</span>
                        <p style="margin:0">Shuo Sun, Malcolm Mielle, Achim J. Lilienthal, Martin Magnusson</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.12535.pdf'>paper</a>
                        <p>We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that provides metrically accurate pose tracking and visually realistic reconstruction.</p>
                    </td>
                </tr>
                <!-- Ha2024RGBD -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">RGBD GS-ICP SLAM</span>
                        <p style="margin:0">Seongbo Ha, Jiung Yeon, Hyeonwoo Yu</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.12550.pdf'>paper</a> / <a href='https://github.com/Lab-of-AI-and-Robotics/GS_ICP_SLAM'>code</a> / <a href='https://youtu.be/e-bHh_uMMxE?si=bU4_Su4J91WQ2MEX'>video</a>
                        <p>Simultaneous Localization and Mapping (SLAM) with dense representation plays a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR) applications.</p>
                    </td>
                </tr>
                <!-- Wang2024EndoGSLAM -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic Surgeries using Gaussian Splatting</span>
                        <p style="margin:0">Kailing Wang, Chen Yang, Yuehao Wang, Sikuang Li, Yan Wang, Qi Dou, Xiaokang Yang, Wei Shen</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://endogslam.loping151.com/'>project page</a> / <a href='https://arxiv.org/pdf/2403.15124.pdf'>paper</a> / <a href='https://github.com/endogslam/EndoGSLAM'>code</a>
                        <p>Precise camera tracking, high-fidelity 3D tissue reconstruction, and real-time online visualization are critical for intrabody medical imaging devices such as endoscopes and capsule robots.</p>
                    </td>
                </tr>
                <!-- Hu2024CGSLAM -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field</span>
                        <p style="margin:0">Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://zju3dv.github.io/cg-slam/'>project page</a> / <a href='https://arxiv.org/pdf/2403.16095'>paper</a> / <a href='https://github.com/hjr37/CG-SLAM'>code</a>
                        <p>Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM).</p>
                    </td>
                </tr>
                <!-- C2024MM3DGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements</span>
                        <p style="margin:0">Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://vita-group.github.io/MM3DGS-SLAM/'>project page</a> / <a href='https://arxiv.org/pdf/2404.00923'>paper</a>
                        <p>Simultaneous localization and mapping is essential for position tracking and scene understanding.</p>
                    </td>
                </tr>
                <!-- Yan2023GSSLAM -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</span>
                        <p style="margin:0">Chi Yan, Delin Qu, Dong Wang, Dan Xu, Zhigang Wang, Bin Zhao, Xuelong Li</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2311.11700.pdf'>paper</a>
                        <p>In this paper, we introduce GS-SLAM that first utilizes 3D Gaussian representation in the Simultaneous Localization and Mapping (SLAM) system.</p>
                    </td>
                </tr>
                <!-- Keetha2023SplaTAM -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM</span>
                        <p style="margin:0">Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang,</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://spla-tam.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2312.02126.pdf'>paper</a> / <a href='https://github.com/spla-tam/SplaTAM'>code</a> / <a href='https://www.youtube.com/watch?v=35SX8DTdQLs'>video</a>
                        <p>Dense simultaneous localization and mapping (SLAM) is pivotal for embodied scene understanding.</p>
                    </td>
                </tr>
                <!-- Matsuki2023Gaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gaussian Splatting SLAM</span>
                        <p style="margin:0">Hidenobu Matsuki, Riku Murai, Paul H. J. Kelly, Andrew J. Davison</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://rmurai.co.uk/projects/GaussianSplattingSLAM/'>project page</a> / <a href='https://www.imperial.ac.uk/media/imperial-college/research-centres-and-groups/dyson-robotics-lab/hide-et-al_GaussianSplattingSLAM_Dec2023.pdf'>paper</a> / <a href='https://github.com/muskie82/MonoGS'>code</a> / <a href='https://youtu.be/x604ghp9R_Q?si=fPtz4kgBKFfcnQf3'>video</a>
                        <p>We present the first application of 3D Gaussian Splatting to incremental 3D reconstruction using a single moving monocular or RGB-D camera.</p>
                    </td>
                </tr>
                <!-- Yugay2023GaussianSLAM -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting</span>
                        <p style="margin:0">Vladimir Yugay, Yue Li, Theo Gevers, Martin R. Oswald</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://vladimiryugay.github.io/gaussian_slam/'>project page</a> / <a href='https://ivi.fnwi.uva.nl/cv/paper/GaussianSLAM.pdf'>paper</a> / <a href='https://github.com/VladimirYugay/Gaussian-SLAM'>code</a> / <a href='https://www.youtube.com/watch?v=RZK1o_ija7M'>video</a>
                        <p>We present the first neural RGBD SLAM method capable of photorealistically reconstructing real-world scenes.</p>
                    </td>
                </tr>
                <!-- Huang2023PhotoSLAM -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras</span>
                        <p style="margin:0">Huajian Huang, Longwei Li, Hui Cheng, Sai-Kit Yeung</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2311.16728.pdf'>paper</a>
                        <p>The integration of neural rendering and the SLAM system recently showed promising results in joint localization and photorealistic view reconstruction.</p>
                    </td>
                </tr>
                <!-- Li2024DNGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization</span>
                        <p style="margin:0">Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu</p>
                        <em>CVPR '24, 2024</em>
                        <br>
                        <a href='https://fictionarry.github.io/DNGaussian/'>project page</a> / <a href='https://arxiv.org/pdf/2403.06912.pdf'>paper</a> / <a href='https://github.com/Fictionarry/DNGaussian'>code</a> / <a href='https://www.youtube.com/watch?v=WKXCFNJHZ4o'>video</a>
                        <p>Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed.</p>
                    </td>
                </tr>
                <!-- Swann2024TouchGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting</span>
                        <p style="margin:0">Aiden Swann, Matthew Strong, Won Kyung Do, Gadiel Sznaier Camps, Mac Schwager, Monroe Kennedy III</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://armlabstanford.github.io/touch-gs'>project page</a> / <a href='https://arxiv.org/pdf/2403.09875.pdf'>paper</a>
                        <p>In this work, we propose a novel method to supervise 3D Gaussian Splatting (3DGS) scenes using optical tactile sensors.</p>
                    </td>
                </tr>
                <!-- Chen2024MVSplat -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</span>
                        <p style="margin:0">Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://donydchen.github.io/mvsplat/'>project page</a> / <a href='https://arxiv.org/pdf/2403.14627'>paper</a> / <a href='https://github.com/donydchen/mvsplat'>code</a>
                        <p>We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images.</p>
                    </td>
                </tr>
                <!-- Wewer2024latentSplat -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction</span>
                        <p style="margin:0">Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, Jan Eric Lenssen</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://geometric-rl.mpi-inf.mpg.de/latentsplat/'>project page</a> / <a href='https://arxiv.org/pdf/2403.16292.pdf'>paper</a>
                        <p>We present latentSplat, a method to predict semantic Gaussians in a 3D latent space that can be splatted and decoded by a light-weight generative 2D architecture.</p>
                    </td>
                </tr>
                <!-- Xu2024GRM -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation</span>
                        <p style="margin:0">Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://justimyhxu.github.io/projects/grm/'>project page</a> / <a href='https://arxiv.org/pdf/2403.14621.pdf'>paper</a> / <a href='https://github.com/justimyhxu/grm'>code</a>
                        <p>We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.</p>
                    </td>
                </tr>
                <!-- Shen2024Gamba -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction</span>
                        <p style="margin:0">Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng Yan, Xinchao Wang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.18795'>paper</a>
                        <p>We tackle the challenge of efficiently reconstructing a 3D asset from a single image with growing demands for automated 3D content creation pipelines.</p>
                    </td>
                </tr>
                <!-- Paliwal2024CoherentGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians</span>
                        <p style="margin:0">Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, Nima Khademi Kalantari</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://people.engr.tamu.edu/nimak/Papers/CoherentGS/index.html'>project page</a> / <a href='https://arxiv.org/pdf/2403.19495'>paper</a>
                        <p>The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS).</p>
                    </td>
                </tr>
                <!-- Zhang2024GaussianCube -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussianCube: Structuring Gaussian Splatting using Optimal Transport for 3D Generative Modeling</span>
                        <p style="margin:0">Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://gaussiancube.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2403.19655.pdf'>paper</a> / <a href='https://github.com/GaussianCube/GaussianCube'>code</a>
                        <p>3D Gaussian Splatting (GS) have achieved considerable improvement over Neural Radiance Fields in terms of 3D fitting fidelity and rendering speed.</p>
                    </td>
                </tr>
                <!-- Fan2024InstantSplat -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds</span>
                        <p style="margin:0">Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://instantsplat.github.io/'>project page</a> / <a href='https://arxiv.org/pdf/2403.20309.pdf'>paper</a>
                        <p>While novel view synthesis (NVS) has made substantial progress in 3D computer vision, it typically requires an initial estimation of camera intrinsics and extrinsics from dense viewpoints.</p>
                    </td>
                </tr>
                <!-- Xiong2023SparseGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SparseGS: Real-Time 360° Sparse View Synthesis using Gaussian Splatting</span>
                        <p style="margin:0">Haolin Xiong, Sairisheek Muttukuru, Rishi Upadhyay, Pradyumna Chari, Achuta Kadambi</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://formycat.github.io/SparseGS-Real-Time-360-Sparse-View-Synthesis-using-Gaussian-Splatting/'>project page</a> / <a href='https://arxiv.org/pdf/2312.00206.pdf'>paper</a>
                        <p>The problem of novel view synthesis has grown significantly in popularity recently with the introduction of Neural Radiance Fields (NeRFs) and other implicit scene representation methods.</p>
                    </td>
                </tr>
                <!-- Zhu2023FSGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</span>
                        <p style="margin:0">Zehao Zhu, Zhiwen Fan, Yifan Jiang, Zhangyang Wang</p>
                        <em>None, 2023</em>
                        <br>
                        <a href='https://zehaozhu.github.io/FSGS/'>project page</a> / <a href='https://arxiv.org/pdf/2312.00451.pdf'>paper</a> / <a href='https://github.com/VITA-Group/FSGS'>code</a>
                        <p>Novel view synthesis from limited observations remains an important and persistent task.</p>
                    </td>
                </tr>
                <!-- Charatan2023pixelSplat -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction</span>
                        <p style="margin:0">David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://davidcharatan.com/pixelsplat/'>project page</a> / <a href='https://arxiv.org/pdf/2312.12337.pdf'>paper</a> / <a href='https://github.com/dcharatan/pixelsplat'>code</a>
                        <p>We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D radiance fields parameterized by 3D Gaussian primitives from pairs of images.</p>
                    </td>
                </tr>
                <!-- Szymanowicz2023Splatter -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Splatter Image: Ultra-Fast Single-View 3D Reconstruction</span>
                        <p style="margin:0">Stanislaw Szymanowicz, Christian Rupprecht, Andrea Vedaldi</p>
                        <em>CVPR '24, 2023</em>
                        <br>
                        <a href='https://szymanowiczs.github.io/splatter-image.html'>project page</a> / <a href='https://arxiv.org/pdf/2312.13150.pdf'>paper</a> / <a href='https://github.com/szymanowiczs/splatter-image'>code</a> / <a href='https://www.youtube.com/watch?v=pcKTf9SVh4g'>video</a>
                        <p>We introduce the Splatter Image, an ultra-fast approach for monocular 3D object reconstruction which operates at 38 FPS.</p>
                    </td>
                </tr>
                <!-- Lei2024GaussNav -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GaussNav: Gaussian Splatting for Visual Navigation</span>
                        <p style="margin:0">Xiaohan Lei, Min Wang, Wengang Zhou, Houqiang Li</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://xiaohanlei.github.io/projects/GaussNav/'>project page</a> / <a href='https://arxiv.org/pdf/2403.11625.pdf'>paper</a> / <a href='https://xiaohanlei.github.io/projects/GaussNav/'>code</a>
                        <p>In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent to locate a specific object depicted in a goal image within an unexplored environment.</p>
                    </td>
                </tr>
                <!-- Jiang20243DGSReLoc -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual ReLocalization</span>
                        <p style="margin:0">Peng Jiang, Gaurav Pandey, Srikanth Saripalli</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.11367'>paper</a>
                        <p>This paper presents a novel system designed for 3D mapping and visual relocalization using 3D Gaussian Splatting.</p>
                    </td>
                </tr>
                <!-- Liu2024Beyond -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Beyond Uncertainty: Risk-Aware Active View Acquisition for Safe Robot Navigation and 3D Scene Understanding with FisherRF</span>
                        <p style="margin:0">Guangyi Liu, Wen Jiang, Boshu Lei, Vivek Pandey, Kostas Daniilidis, Nader Motee</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.11396'>paper</a>
                        <p>This work proposes a novel approach to bolster both the robot's risk assessment and safety measures while deepening its understanding of 3D scenes, which is achieved by leveraging Radiance Field (RF) models and 3D Gaussian Splatting.</p>
                    </td>
                </tr>
                <!-- Herau20243DGSCalib -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration</span>
                        <p style="margin:0">Quentin Herau, Moussab Bennehar, Arthur Moreau, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.11577'>paper</a>
                        <p>Reliable multimodal sensor fusion algorithms re- quire accurate spatiotemporal calibration.</p>
                    </td>
                </tr>
                <!-- Zhou2024HUGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting</span>
                        <p style="margin:0">Hongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao</p>
                        <em>CVPR '24, 2024</em>
                        <br>
                        <a href='https://xdimlab.github.io/hugs_website/'>project page</a> / <a href='https://arxiv.org/pdf/2403.12722.pdf'>paper</a>
                        <p>Holistic understanding of urban scenes based on RGB images is a challenging yet important problem.</p>
                    </td>
                </tr>
                <!-- Li2024HOGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">HO-Gaussian: Hybrid Optimization of 3D Gaussian Splatting for Urban Scenes</span>
                        <p style="margin:0">Zhuopeng Li, Yilin Zhang, Chenming Wu, Jianke Zhu, Liangjun Zhang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.20032.pdf'>paper</a>
                        <p>The rapid growth of 3D Gaussian Splatting (3DGS) has revolutionized neural rendering, enabling real-time production of high-quality renderings.</p>
                    </td>
                </tr>
                <!-- Yu2024SGD -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior</span>
                        <p style="margin:0">Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.20079.pdf'>paper</a>
                        <p>Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation.</p>
                    </td>
                </tr>
                <!-- Li2024GGRt -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GGRt: Towards Generalizable 3D Gaussians without Pose Priors in Real-Time</span>
                        <p style="margin:0">Hao Li, Yuanyuan Gao, Dingwen Zhang, Chenming Wu, Yalun Dai, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://3d-aigc.github.io/GGRt/'>project page</a> / <a href='https://arxiv.org/pdf/2403.10147'>paper</a>
                        <p>This paper presents GGRt, a novel approach to generalizable novel view synthesis that alleviates the need for real camera poses, complexity in processing high-resolution images, and lengthy optimization processes, thus facilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in real-world scenarios.</p>
                    </td>
                </tr>
                <!-- Cai2024GSPose -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GS-Pose: Cascaded Framework for Generalizable Segmentation-based 6D Object Pose Estimation</span>
                        <p style="margin:0">Dingding Cai, Janne Heikkilä, Esa Rahtu</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://dingdingcai.github.io/gs-pose/'>project page</a> / <a href='https://arxiv.org/pdf/2403.10683'>paper</a> / <a href='https://github.com/dingdingcai/GS-pose'>code</a> / <a href='https://youtu.be/SnJazusDLM8'>video</a>
                        <p>This paper introduces GS-Pose, an end-to-end framework for locating and estimating the 6D pose of objects.</p>
                    </td>
                </tr>
                <!-- Suzuki2024Fed3DGS -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning</span>
                        <p style="margin:0">Teppei Suzuki</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.11460'>paper</a> / <a href='https://github.com/dingdingcai/GS-pose'>code</a>
                        <p>In this work, we present Fed3DGS, a scalable 3D reconstruction framework based on 3D Gaussian splatting (3DGS) with federated learning.</p>
                    </td>
                </tr>
                <!-- Cai2024GSPose -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">GS-Pose: Cascaded Framework for Generalizable Segmentation-based 6D Object Pose Estimation</span>
                        <p style="margin:0">Dingding Cai, Janne Heikkilä, Esa Rahtu</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://dingdingcai.github.io/gs-pose/'>project page</a> / <a href='https://arxiv.org/pdf/2403.10683'>paper</a> / <a href='https://github.com/dingdingcai/GS-pose'>code</a> / <a href='https://youtu.be/SnJazusDLM8'>video</a>
                        <p>This paper introduces GS-Pose, an end-to-end framework for locating and estimating the 6D pose of objects.</p>
                    </td>
                </tr>
                <!-- Tarun2024Creating -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Creating Seamless 3D Maps Using Radiance Fields</span>
                        <p style="margin:0">Sai Tarun Sathyan, Thomas B. Kinsman</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.11364.pdf'>paper</a>
                        <p>It is desirable to create 3D object models and 3D maps from 2D input images for applications such as navigation, virtual tourism, and urban planning.</p>
                    </td>
                </tr>
                <!-- Wu2024HGSMapping -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation in Urban Scenes</span>
                        <p style="margin:0">Ke Wu, Kaizhao Zhang, Zhiwei Zhang, Shanshuai Yuan, Muer Tie, Julong Wei, Zijun Xu, Jieru Zhao, Zhongxue Gan, Wenchao Ding</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://arxiv.org/pdf/2403.20159.pdf'>paper</a>
                        <p>Online dense mapping of urban scenes forms a fundamental cornerstone for scene understanding and navigation of autonomous vehicles.</p>
                    </td>
                </tr>
                <!-- Liu2024CityGaussian -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='assets/no_img_ph.jpg' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">CityGaussian: Real-time High-quality Large-Scale Scene Rendering with Gaussians</span>
                        <p style="margin:0">Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang</p>
                        <em>None, 2024</em>
                        <br>
                        <a href='https://dekuliutesla.github.io/citygs/'>project page</a> / <a href='https://arxiv.org/pdf/2403.20159.pdf'>paper</a>
                        <p>The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS).</p>
                    </td>
                </tr>
    
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                    This page was inspired by this <a href="https://github.com/jonbarron/jonbarron_website">template</a> created by <a href="https://jonbarron.info/">Jon Barron</a>.
                </p>
            </td>
        </tr>
        </tbody></table>
    </td>
</tr>
</table>
</body>
</html>
